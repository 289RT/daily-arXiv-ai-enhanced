<div id=toc></div>

# Table of Contents

- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [1] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: 该论文提出了一种基于CLIP风格的多模态框架，用于对齐3D蛋白质结构与功能注释，并通过对比学习提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 通过结合视觉语言模型的最新进展，旨在从大规模蛋白质数据集中检索结构和语义相似的蛋白质，以促进蛋白质功能的理解。

Method: 使用对比学习框架，构建了一个包含约20万蛋白质-描述对的数据集，并评估了模型在PDB和EMDB数据集上的跨数据库检索能力。

Result: 模型在零样本检索任务中表现出色，展示了多模态基础模型在蛋白质结构-功能理解中的潜力。

Conclusion: 该研究表明多模态模型可以有效提升蛋白质结构-功能关联的检索性能，为蛋白质生物学提供了新的工具。

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


### [2] [Diffusion Sequence Models for Enhanced Protein Representation and Generation](https://arxiv.org/abs/2506.08293)
*Logan Hallee,Nikolaos Rafailidis,David B. Bichara,Jason P. Gleghorn*

Main category: q-bio.BM

TL;DR: 本文介绍了扩散序列模型（DSM），一种通过掩蔽扩散训练的新型蛋白质语言模型（pLM），能够在表示学习和生成蛋白质设计中表现优异。


<details>
  <summary>Details</summary>
Motivation: 蛋白质在生物学中具有广泛的应用潜力，但现有pLM模型的生成能力有限，因此需要一种能够统一表示学习和生成任务的新方法。

Method: DSM基于ESM2架构，结合了LLaDA框架中的掩蔽前向扩散过程，训练后能生成高质量的蛋白质序列。

Result: DSM在生成多样且仿生序列时表现出色，即使在高噪声条件下也能保持结构一致性和功能预测能力。其变体DSM(ppi)在蛋白质结合任务中表现优于已知结合剂。

Conclusion: 掩蔽扩散是一种强大的范式，能够在单一框架中统一蛋白质的表示学习和生成任务。

Abstract: Proteins are fundamental to biology, executing diverse functions through
complex physicochemical interactions, and they hold transformative potential
across medicine, materials science, and environmental applications. Protein
Language Models (pLMs) aim to unlock insights from the vast space of unlabeled
protein sequences by learning rich, semantic representations from primary
sequences via masked language modeling. However, these models typically exhibit
limited generative capacity. In this work, we introduce the Diffusion Sequence
Model (DSM), a novel pLM trained with masked diffusion to enable both
high-quality representation learning and generative protein design. DSM builds
upon the ESM2 architecture by incorporating a masked forward diffusion process
inspired by the LLaDA framework. After training, DSM is capable of generating
diverse, biomimetic sequences that align with expected amino acid compositions,
secondary structures, and predicted functions, even with 90\% token corruption.
Furthermore, DSM's learned representations match or exceed those of similarly
sized pLMs on downstream tasks. We also introduce DSM(ppi), a variant
fine-tuned to generate protein binders by attending to target sequences. We
demonstrate DSM(ppi)'s effectiveness on the challenging Bench-tested Binder
Benchmark (BenchBB), where both DSM and DSM(ppi) produce candidates with
superior predicted binding affinity compared to known binders. Our results
establish masked diffusion as a powerful paradigm for unifying protein
representation and generation in a single framework.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan,Zhenxiao Cao,Zhangyang Gao,Siyuan Li,Yufei Huang,Stan Z. Li*

Main category: cs.LG

TL;DR: AFDB的结构数据在训练深度模型时存在几何偏差问题，作者提出DeSAE模型对AFDB结构进行去偏处理，显著提升了逆折叠等任务的性能。


<details>
  <summary>Details</summary>
Motivation: AFDB结构数据虽然覆盖广泛且精度高，但其几何偏差限制了在逆折叠等任务中的直接应用，作者希望通过去偏方法提升其适用性。

Method: 提出DeSAE（去偏结构自编码器），通过学习从故意破坏的主链几何结构中重建自然构象，从而捕获更鲁棒的结构流形。

Result: 应用DeSAE处理后，AFDB结构的去偏效果显著，逆折叠任务在多个基准测试中性能提升。

Conclusion: 研究表明预测结构中细微的系统偏差对性能有重要影响，DeSAE为解决此类问题提供了有效框架。

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>
