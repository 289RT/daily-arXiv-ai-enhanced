{"id": "2506.14796", "pdf": "https://arxiv.org/pdf/2506.14796", "abs": "https://arxiv.org/abs/2506.14796", "authors": ["Zhangyang Gao", "Hao Wang", "Cheng Tan", "Chenrui Xu", "Mengdi Liu", "Bozhen Hu", "Linlin Chao", "Xiaoming Zhang", "Stan Z. Li"], "title": "PFMBench: Protein Foundation Model Benchmark", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": null, "summary": "This study investigates the current landscape and future directions of protein foundation model research. While recent advancements have transformed protein science and engineering, the field lacks a comprehensive benchmark for fair evaluation and in-depth understanding. Since ESM-1B, numerous protein foundation models have emerged, each with unique datasets and methodologies. However, evaluations often focus on limited tasks tailored to specific models, hindering insights into broader generalization and limitations. Specifically, researchers struggle to understand the relationships between tasks, assess how well current models perform across them, and determine the criteria in developing new foundation models. To fill this gap, we present PFMBench, a comprehensive benchmark evaluating protein foundation models across 38 tasks spanning 8 key areas of protein science. Through hundreds of experiments on 17 state-of-the-art models across 38 tasks, PFMBench reveals the inherent correlations between tasks, identifies top-performing models, and provides a streamlined evaluation protocol. Code is available at \\href{https://github.com/biomap-research/PFMBench}{\\textcolor{blue}{GitHub}}."}
{"id": "2506.15309", "pdf": "https://arxiv.org/pdf/2506.15309", "abs": "https://arxiv.org/abs/2506.15309", "authors": ["JÃºlia Vilalta-Mor", "Alexis Molina", "Laura Ortega Varga", "Isaac Filella-Merce", "Victor Guallar"], "title": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "16 pages, 7 figures", "summary": "Simultaneously optimizing molecules against multiple therapeutic targets remains a profound challenge in drug discovery, particularly due to sparse rewards and conflicting design constraints. We propose a structured active learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational autoencoder (VAE) into iterative loops designed to balance chemical diversity, molecular quality, and multi-target affinity. Our method alternates between expanding chemically feasible regions of latent space and progressively constraining molecules based on increasingly stringent multi-target docking thresholds. In a proof-of-concept study targeting three related coronavirus main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently generated a structurally diverse set of pan-inhibitor candidates. We demonstrate that careful timing and strategic placement of chemical filters within this active learning pipeline markedly enhance exploration of beneficial chemical space, transforming the sparse-reward, multi-objective drug design problem into an accessible computational task. Our framework thus provides a generalizable roadmap for efficiently navigating complex polypharmacological landscapes."}
{"id": "2506.15585", "pdf": "https://arxiv.org/pdf/2506.15585", "abs": "https://arxiv.org/abs/2506.15585", "authors": ["Robert Welch", "Charles Laughton", "Oliver Henrich", "Tom Burnley", "Daniel Cole", "Alan Real", "Sarah Harris", "James Gebbie-Rayet"], "title": "Engineering Supercomputing Platforms for Biomolecular Applications", "categories": ["physics.bio-ph", "q-bio.BM"], "comment": "54 pages, 55 figures. Benchmarking/method/software by Robert Welch. MIG Benchmarks by Charles Laughton. Sections 1-6 by Robert Welch, 7 by James Gebbie-Rayet & Robert Welch, 3.8 by Oliver Henrich. Edited by James Gebbie-Rayet, Tom Burnley, Daniel Cole, Alan Real, Sarah Harris, Mark Wilkinson. Methods: github.com/HECBioSim/hpcbench. Raw data: github.com/HECBioSim/benchmark-results", "summary": "A range of computational biology software (GROMACS, AMBER, NAMD, LAMMPS, OpenMM, Psi4 and RELION) was benchmarked on a representative selection of HPC hardware, including AMD EPYC 7742 CPU nodes, NVIDIA V100 and AMD MI250X GPU nodes, and an NVIDIA GH200 testbed. The raw performance, power efficiency and data storage requirements of the software was evaluated for each HPC facility, along with qualitative factors such as the user experience and software environment. It was found that the diversity of methods used within computational biology means that there is no single HPC hardware that can optimally run every type of HPC job, and that diverse hardware is the only way to properly support all methods. New hardware, such as AMD GPUs and Nvidia AI chips, are mostly compatible with existing methods, but are also more labour-intensive to support. GPUs offer the most efficient way to run most computational biology tasks, though some tasks still require CPUs. A fast HPC node running molecular dynamics can produce around 10GB of data per day, however, most facilities and research institutions lack short-term and long-term means to store this data. Finally, as the HPC landscape has become more complex, deploying software and keeping HPC systems online has become more difficult. This situation could be improved through hiring/training in DevOps practices, expanding the consortium model to provide greater support to HPC system administrators, and implementing build frameworks/containerisation/virtualisation tools to allow users to configure their own software environment, rather than relying on centralised software installations."}
