{"id": "2510.09668", "pdf": "https://arxiv.org/pdf/2510.09668", "abs": "https://arxiv.org/abs/2510.09668", "authors": ["Maryam Abdollahi Shamami", "Babak Teimourpour", "Farshad Sharifi"], "title": "A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Drug-drug interactions (DDIs) are a leading cause of preventable adverse\nevents, often complicating treatment and increasing healthcare costs. At the\nsame time, knowing which drugs do not interact is equally important, as such\nknowledge supports safer prescriptions and better patient outcomes. In this\nstudy, we propose an interpretable and efficient framework that blends modern\nmachine learning with domain knowledge to improve DDI prediction. Our approach\ncombines two complementary molecular embeddings - Mol2Vec, which captures\nfragment-level structural patterns, and SMILES-BERT, which learns contextual\nchemical features - together with a leakage-free, rule-based clinical score\n(RBScore) that injects pharmacological knowledge without relying on interaction\nlabels. A lightweight neural classifier is then optimized using a novel\nthree-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global\nexploration and local refinement for stable performance. Experiments on\nreal-world datasets demonstrate that the model achieves high predictive\naccuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a\nclinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,\nstudies show how embedding fusion, RBScore, and the optimizer each contribute\nto precision and robustness. Together, these results highlight a practical\npathway for building reliable, interpretable, and computationally efficient\nmodels that can support safer drug therapies and clinical decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5206\u5b50\u5d4c\u5165\u548c\u4e34\u5e8a\u8bc4\u5206\u7684\u53ef\u89e3\u91ca\u9ad8\u6548\u6846\u67b6\u6765\u9884\u6d4b\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u662f\u5bfc\u81f4\u4e0d\u826f\u53cd\u5e94\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4e86\u89e3\u836f\u7269\u662f\u5426\u76f8\u4e92\u4f5c\u7528\u5bf9\u5b89\u5168\u7528\u836f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408Mol2Vec\u548cSMILES-BERT\u5206\u5b50\u5d4c\u5165\uff0c\u5f15\u5165\u65e0\u6cc4\u6f0f\u7684\u4e34\u5e8a\u8bc4\u5206RBScore\uff0c\u5e76\u91c7\u7528\u4e09\u9636\u6bb5\u4f18\u5316\u7b56\u7565RSmpl-ACO-PSO\u8bad\u7ec3\u5206\u7c7b\u5668\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff08ROC-AUC 0.911\uff09\uff0c\u5728\u7cd6\u5c3f\u75c5\u961f\u5217\u4e2d\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2510.10020", "pdf": "https://arxiv.org/pdf/2510.10020", "abs": "https://arxiv.org/abs/2510.10020", "authors": ["Henry D. Smith", "Nathaniel L. Diamant", "Brian L. Trippe"], "title": "Calibrating Generative Models", "categories": ["stat.ML", "cs.LG", "q-bio.BM"], "comment": "Our codebase accompanying the paper is available at:\n  https://github.com/smithhenryd/cgm", "summary": "Generative models frequently suffer miscalibration, wherein class\nprobabilities and other statistics of the sampling distribution deviate from\ndesired values. We frame calibration as a constrained optimization problem and\nseek the closest model in Kullback-Leibler divergence satisfying calibration\nconstraints. To address the intractability of imposing these constraints\nexactly, we introduce two surrogate objectives for fine-tuning: (1) the relax\nloss, which replaces the constraint with a miscalibration penalty, and (2) the\nreward loss, which converts calibration into a reward fine-tuning problem. We\ndemonstrate that these approaches substantially reduce calibration error across\nhundreds of simultaneous constraints and models with up to one billion\nparameters, spanning applications in protein design, image generation, and\nlanguage modeling.", "AI": {"tldr": "\u751f\u6210\u6a21\u578b\u5e38\u9762\u4e34\u6821\u51c6\u95ee\u9898\uff0c\u672c\u6587\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u548c\u4e24\u79cd\u66ff\u4ee3\u76ee\u6807\uff08relax loss\u548creward loss\uff09\u663e\u8457\u964d\u4f4e\u6821\u51c6\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u6982\u7387\u7edf\u8ba1\u4e0e\u671f\u671b\u503c\u504f\u79bb\u7684\u6821\u51c6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eKullback-Leibler\u6563\u5ea6\u7684\u7ea6\u675f\u4f18\u5316\uff0c\u5e76\u5f15\u5165relax loss\u548creward loss\u4e24\u79cd\u66ff\u4ee3\u76ee\u6807\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u3001\u56fe\u50cf\u751f\u6210\u548c\u8bed\u8a00\u6a21\u578b\u7b49\u5e94\u7528\u4e2d\uff0c\u5927\u5e45\u964d\u4f4e\u6570\u767e\u4e2a\u540c\u65f6\u7ea6\u675f\u548c\u5341\u4ebf\u53c2\u6570\u6a21\u578b\u7684\u6821\u51c6\u8bef\u5dee\u3002", "conclusion": "\u4e24\u79cd\u66ff\u4ee3\u76ee\u6807\u6709\u6548\u63d0\u5347\u751f\u6210\u6a21\u578b\u7684\u6821\u51c6\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5927\u89c4\u6a21\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.11188", "pdf": "https://arxiv.org/pdf/2510.11188", "abs": "https://arxiv.org/abs/2510.11188", "authors": ["Xinhui Chen", "Zuchao Li", "Mengqi Gao", "Yufeng Zhang", "Chak Tou Leong", "Haoyang Li", "Jiaqi Chen"], "title": "Protein as a Second Language for LLMs", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "Main paper: 9 pages, 6 figures. With references and appendix: 18\n  pages, 9 figures total. Submitted to ICLR 2026 (under review)", "summary": "Deciphering the function of unseen protein sequences is a fundamental\nchallenge with broad scientific impact, yet most existing methods depend on\ntask-specific adapters or large-scale supervised fine-tuning. We introduce the\n\"Protein-as-Second-Language\" framework, which reformulates amino-acid sequences\nas sentences in a novel symbolic language that large language models can\ninterpret through contextual exemplars. Our approach adaptively constructs\nsequence-question-answer triples that reveal functional cues in a zero-shot\nsetting, without any further training. To support this process, we curate a\nbilingual corpus of 79,926 protein-QA instances spanning attribute prediction,\ndescriptive understanding, and extended reasoning. Empirically, our method\ndelivers consistent gains across diverse open-source LLMs and GPT-4, achieving\nup to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned\nprotein-specific language models. These results highlight that generic LLMs,\nwhen guided with protein-as-language cues, can outperform domain-specialized\nmodels, offering a scalable pathway for protein understanding in foundation\nmodels.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a'Protein-as-Second-Language'\u7684\u6846\u67b6\uff0c\u5c06\u86cb\u767d\u8d28\u5e8f\u5217\u8f6c\u5316\u4e3a\u8bed\u8a00\u6a21\u578b\u53ef\u7406\u89e3\u7684\u53e5\u5b50\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u4efb\u52a1\u5b9e\u73b0\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u6216\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u901a\u7528\u8bed\u8a00\u6a21\u578b\u5728\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u5e8f\u5217-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u6784\u5efa\u53cc\u8bed\u8bed\u6599\u5e93\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u9884\u6d4b\u3002", "result": "\u5728\u591a\u79cdLLM\u548cGPT-4\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u63d0\u53477%\uff08\u6700\u9ad817.2%\uff09\uff0c\u8d85\u8d8a\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "\u901a\u7528\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u86cb\u767d\u8d28\u8bed\u8a00\u7ebf\u7d22\u53ef\u8d85\u8d8a\u4e13\u7528\u6a21\u578b\uff0c\u4e3a\u86cb\u767d\u8d28\u7406\u89e3\u63d0\u4f9b\u53ef\u6269\u5c55\u9014\u5f84\u3002"}}
