{"id": "2512.17169", "pdf": "https://arxiv.org/pdf/2512.17169", "abs": "https://arxiv.org/abs/2512.17169", "authors": ["Nalin Arora", "Aviral Chauhan", "Siddhant Rana", "Mahansh Aditya", "Sumit Bhagat", "Aditya Kumar", "Akash Kumar", "Akanksh Semar", "Ayush Vikram Singh", "Ganesh Bagler"], "title": "Application of machine learning to predict food processing level using Open Food Facts", "categories": ["q-bio.BM", "cs.LG"], "comment": "27 Pages (22 Pages of Main Manuscript + Supplementary Material), 7 Figures, 1 Table", "summary": "Ultra-processed foods are increasingly linked to health issues like obesity, cardiovascular disease, type 2 diabetes, and mental health disorders due to poor nutritional quality. This first-of-its-kind study at such a scale uses machine learning to classify food processing levels (NOVA) based on the Open Food Facts dataset of over 900,000 products. Models including LightGBM, Random Forest, and CatBoost were trained on nutrient concentration data. LightGBM performed best, achieving 80-85% accuracy across different nutrient panels and effectively distinguishing minimally from ultra-processed foods. Exploratory analysis revealed strong associations between higher NOVA classes and lower Nutri-Scores, indicating poorer nutritional quality. Products in NOVA 3 and 4 also had higher carbon footprints and lower Eco-Scores, suggesting greater environmental impact. Allergen analysis identified gluten and milk as common in ultra-processed items, posing risks to sensitive individuals. Categories like Cakes and Snacks were dominant in higher NOVA classes, which also had more additives, highlighting the role of ingredient modification. This study, leveraging the largest dataset of NOVA-labeled products, emphasizes the health, environmental, and allergenic implications of food processing and showcases machine learning's value in scalable classification. A user-friendly web tool is available for NOVA prediction using nutrient data: https://cosylab.iiitd.edu.in/foodlabel/."}
{"id": "2512.17815", "pdf": "https://arxiv.org/pdf/2512.17815", "abs": "https://arxiv.org/abs/2512.17815", "authors": ["Xinyan Zhao", "Yi-Ching Tang", "Rivaaj Monsia", "Victor J. Cantu", "Ashwin Kumar Ramesh", "Xiaozhong Liu", "Zhiqiang An", "Xiaoqian Jiang", "Yejin Kim"], "title": "Structure-Aware Antibody Design with Affinity-Optimized Inverse Folding", "categories": ["cs.CE", "q-bio.BM"], "comment": null, "summary": "Motivation: The clinical efficacy of antibody therapeutics critically depends on high-affinity target engagement, yet laboratory affinity-maturation campaigns are slow and costly. In computational settings, most protein language models (PLMs) are not trained to favor high-affinity antibodies, and existing preference optimization approaches introduce substantial computational overhead without clear affinity gains. Therefore, this work proposes SimBinder-IF, which converts the inverse folding model ESM-IF into an antibody sequence generator by freezing its structure encoder and training only its decoder to prefer experimentally stronger binders through preference optimization.\n  Results: On the 11-assay AbBiBench benchmark, SimBinder-IF achieves a 55 percent relative improvement in mean Spearman correlation between log-likelihood scores and experimentally measured binding affinity compared to vanilla ESM-IF (from 0.264 to 0.410). In zero-shot generalization across four unseen antigen-antibody complexes, the correlation improves by 156 percent (from 0.115 to 0.294). SimBinder-IF also outperforms baselines in top-10 precision for ten-fold or greater affinity improvements. A case study redesigning antibody F045-092 for A/California/04/2009 (pdmH1N1) shows that SimBinder-IF proposes variants with substantially lower predicted binding free energy changes than ESM-IF (mean Delta Delta G -75.16 vs -46.57). Notably, SimBinder-IF trains only about 18 percent of the parameters of the full ESM-IF model, highlighting its parameter efficiency for high-affinity antibody generation."}
