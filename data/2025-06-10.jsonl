{"id": "2506.06305", "pdf": "https://arxiv.org/pdf/2506.06305", "abs": "https://arxiv.org/abs/2506.06305", "authors": ["Noémie Bergues", "Arthur Carré", "Paul Join-Lambert", "Brice Hoffmann", "Arnaud Blondel", "Hamza Tajmouati"], "title": "Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Predicting the 3D conformation of small molecules within protein binding\nsites is a key challenge in drug design. When a crystallized reference ligand\n(template) is available, it provides geometric priors that can guide 3D pose\nprediction. We present a two-stage method for ligand conformation generation\nguided by such templates. In the first stage, we introduce a molecular\nalignment approach based on flow-matching to generate 3D coordinates for the\nligand, using the template structure as a reference. In the second stage, a\ndifferentiable pose optimization procedure refines this conformation based on\nshape and pharmacophore similarities, internal energy, and, optionally, the\nprotein binding pocket. We evaluate our approach on a new benchmark of ligand\npairs co-crystallized with the same target and show that it outperforms\nstandard docking tools and open-access alignment methods, especially in cases\ninvolving low similarity to the template or high ligand flexibility."}
{"id": "2506.06673", "pdf": "https://arxiv.org/pdf/2506.06673", "abs": "https://arxiv.org/abs/2506.06673", "authors": ["Zifang He", "Longtao Yang", "Peiyao Ma"], "title": "NSUN2 as a potential prognostic as well as therapeutic target in cancer by regulating m5C modification", "categories": ["q-bio.BM"], "comment": null, "summary": "m5C modification is a type of RNA methylation modification, and its major\nmethyltransferase, NSUN2, catalyzes m5C modification. NSUN2 is overexpressed in\na variety of cancers, and it affects the metabolism of RNA from target genes by\naffecting the level of m5C modification in cancer cells, which in turn promotes\nthe development of cancers and is associated with poor prognosis. This review\nsummarizes the mechanisms by which NSUN2 and m5C play a pro-cancer role in\nvarious cancers, and the relationship between NSUN2 and the prognosis of\nvarious cancers, with the aim of identifying NSUN2 as a prognostic indicator\nand a target for future cancer therapy, and to provide a clearer therapeutic\nidea and direction for the future treatment of cancer."}
{"id": "2506.06915", "pdf": "https://arxiv.org/pdf/2506.06915", "abs": "https://arxiv.org/abs/2506.06915", "authors": ["Odin Zhang", "Haitao Lin", "Xujun Zhang", "Xiaorui Wang", "Zhenxing Wu", "Qing Ye", "Weibo Zhao", "Jike Wang", "Kejun Ying", "Yu Kang", "Chang-yu Hsieh", "Tingjun Hou"], "title": "Graph Neural Networks in Modern AI-aided Drug Discovery", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Graph neural networks (GNNs), as topology/structure-aware models within deep\nlearning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By\ndirectly operating on molecular graphs, GNNs offer an intuitive and expressive\nframework for learning the complex topological and geometric features of\ndrug-like molecules, cementing their role in modern molecular modeling. This\nreview provides a comprehensive overview of the methodological foundations and\nrepresentative applications of GNNs in drug discovery, spanning tasks such as\nmolecular property prediction, virtual screening, molecular generation,\nbiomedical knowledge graph construction, and synthesis planning. Particular\nattention is given to recent methodological advances, including geometric GNNs,\ninterpretable models, uncertainty quantification, scalable graph architectures,\nand graph generative frameworks. We also discuss how these models integrate\nwith modern deep learning approaches, such as self-supervised learning,\nmulti-task learning, meta-learning and pre-training. Throughout this review, we\nhighlight the practical challenges and methodological bottlenecks encountered\nwhen applying GNNs to real-world drug discovery pipelines, and conclude with a\ndiscussion on future directions."}
{"id": "2506.07035", "pdf": "https://arxiv.org/pdf/2506.07035", "abs": "https://arxiv.org/abs/2506.07035", "authors": ["Zixuan Jiang", "Renjing Xu"], "title": "AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Deciphering protein function remains a fundamental challenge in protein\nrepresentation learning. The task presents significant difficulties for protein\nlanguage models (PLMs) due to the sheer volume of functional annotation\ncategories and the highly imbalanced distribution of annotated instances across\nbiological ontologies. Inspired by the remarkable success of reinforcement\nlearning from human feedback (RLHF) in large language model (LLM) alignment, we\npropose AnnoDPO, a novel multi-modal framework for protein function prediction\nthat leverages Direct Preference Optimization (DPO) to enhance annotation\nlearning. Our methodology addresses the dual challenges of annotation scarcity\nand category imbalance through preference-aligned training objectives,\nestablishing a new paradigm for biological knowledge integration in protein\nrepresentation learning."}
{"id": "2506.06294", "pdf": "https://arxiv.org/pdf/2506.06294", "abs": "https://arxiv.org/abs/2506.06294", "authors": ["Yunqing Liu", "Wenqi Fan", "Xiaoyong Wei", "Qing Li"], "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Proteins are central to biological systems, participating as building blocks\nacross all forms of life. Despite advancements in understanding protein\nfunctions through protein sequence analysis, there remains potential for\nfurther exploration in integrating protein structural information. We argue\nthat the structural information of proteins is not only limited to their 3D\ninformation but also encompasses information from amino acid molecules (local\ninformation) to protein-protein structure similarity (global information). To\naddress this, we propose \\textbf{GLProtein}, the first framework in protein\npre-training that incorporates both global structural similarity and local\namino acid details to enhance prediction accuracy and functional insights.\nGLProtein innovatively combines protein-masked modelling with triplet structure\nsimilarity scoring, protein 3D distance encoding and substructure-based amino\nacid molecule encoding. Experimental results demonstrate that GLProtein\noutperforms previous methods in several bioinformatics tasks, including\npredicting protein-protein interaction, contact prediction, and so on."}
{"id": "2506.06443", "pdf": "https://arxiv.org/pdf/2506.06443", "abs": "https://arxiv.org/abs/2506.06443", "authors": ["Luis Pinto"], "title": "Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "comment": null, "summary": "Pretrained molecular encoders have become indispensable in computational\nchemistry for tasks such as property prediction and molecular generation.\nHowever, the standard practice of relying solely on final-layer embeddings for\ndownstream tasks may discard valuable information. In this work, we challenge\nthis convention by conducting a comprehensive layer-wise analysis of five\ndiverse molecular encoders across 22 ADMET property prediction tasks. Our\nresults demonstrate that embeddings from intermediate layers consistently\noutperform final-layer representations. Specifically, using fixed embeddings\nfrom the optimal intermediate layers improved downstream performance by an\naverage of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to\nthese intermediate layers yielded even greater average improvements of 8.5%,\nwith performance increases as high as 40.8%, achieving new state-of-the-art\nresults on several benchmarks. Additionally, a strong positive correlation\nbetween fixed embedding performance and finetuning outcomes supports an\nefficient evaluate-then-finetune approach, enabling identification of optimal\nlayers with reduced computational cost. These findings highlight the importance\nof exploring the full representational depth of molecular encoders to achieve\nsubstantial performance improvements and computational efficiency. The code is\nmade publicly available at\nhttps://github.com/luispintoc/Unlocking-Chemical-Insights."}
{"id": "2506.06701", "pdf": "https://arxiv.org/pdf/2506.06701", "abs": "https://arxiv.org/abs/2506.06701", "authors": ["Fudong Lin", "Wanrou Du", "Jinchan Liu", "Tarikul Milon", "Shelby Meche", "Wu Xu", "Xiaoqi Qin", "Xu Yuan"], "title": "Do Protein Transformers Have Biological Intelligence?", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "Accepted by European Conference on Machine Learning and Principles\n  and Practice of Knowledge Discovery in Databases (ECML-PKDD 2025)", "summary": "Deep neural networks, particularly Transformers, have been widely adopted for\npredicting the functional properties of proteins. In this work, we focus on\nexploring whether Protein Transformers can capture biological intelligence\namong protein sequences. To achieve our goal, we first introduce a protein\nfunction dataset, namely Protein-FN, providing over 9000 protein data with\nmeaningful labels. Second, we devise a new Transformer architecture, namely\nSequence Protein Transformers (SPT), for computationally efficient protein\nfunction predictions. Third, we develop a novel Explainable Artificial\nIntelligence (XAI) technique called Sequence Score, which can efficiently\ninterpret the decision-making processes of protein models, thereby overcoming\nthe difficulty of deciphering biological intelligence bided in Protein\nTransformers. Remarkably, even our smallest SPT-Tiny model, which contains only\n5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%\non the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,\nall accomplished by training from scratch. Besides, our Sequence Score\ntechnique helps reveal that our SPT models can discover several meaningful\npatterns underlying the sequence structures of protein data, with these\npatterns aligning closely with the domain knowledge in the biology community.\nWe have officially released our Protein-FN dataset on Hugging Face Datasets\nhttps://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at\nhttps://github.com/fudong03/BioIntelligence."}
