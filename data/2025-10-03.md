<div id=toc></div>

# Table of Contents

- [q-bio.BM](#q-bio.BM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [1] [BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning](https://arxiv.org/abs/2510.01632)
*Xin Wang,Carlos Oliver*

Main category: q-bio.BM

TL;DR: BioBlobs是一种动态分区蛋白质结构为灵活大小的子结构的模块，提高了蛋白质编码器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质表示学习模型依赖刚性子结构，扭曲了功能相关的信号。

Method: BioBlobs动态分区蛋白质结构为非重叠子结构，并通过共享代码簿量化。

Result: BioBlobs提升了多种蛋白质表示学习任务的性能。

Conclusion: 直接捕获功能相关子结构的架构能提升预测性能并提供蛋白质功能的机制洞察。

Abstract: Protein function is driven by coherent substructures which vary in size and
topology, yet current protein representation learning models (PRL) distort
these signals by relying on rigid substructures such as k-hop and fixed radius
neighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable
module that represents proteins by dynamically partitioning structures into
flexibly-sized, non-overlapping substructures ("blobs"). The resulting blobs
are quantized into a shared and interpretable codebook, yielding a discrete
vocabulary of function-relevant protein substructures used to compute protein
embeddings. We show that BioBlobs representations improve the performance of
widely used protein encoders such as GVP-GNN across various PRL tasks. Our
approach highlights the value of architectures that directly capture
function-relevant protein substructures, enabling both improved predictive
performance and mechanistic insight into protein function.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [2] [Folding lattice proteins confined on minimal grids using a quantum-inspired encoding](https://arxiv.org/abs/2510.01890)
*Anders Irbäck,Lucas Knuthson,Sandipan Mohanty*

Main category: quant-ph

TL;DR: 该论文探讨了如何在密集蛋白质系统中解决位阻碰撞问题，通过将最小能量寻找问题转化为QUBO形式，展示了经典模拟退火和量子-经典混合退火方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统显式链方法在探索密集蛋白质系统时面临位阻碰撞的挑战，本研究旨在通过优化方法解决这一问题。

Method: 将最小能量寻找问题转化为QUBO形式，并分别采用经典模拟退火、量子-经典混合退火、以及线性与二次规划方法进行求解。

Result: 经典模拟退火和量子-经典混合退火方法在链长为48时能够快速且一致地解决问题，而线性与二次规划方法在链约束下表现不佳。

Conclusion: QUBO形式的优化问题适合通过经典和量子退火方法高效求解，为密集蛋白质系统的研究提供了新途径。

Abstract: Steric clashes pose a challenge when exploring dense protein systems using
conventional explicit-chain methods. A minimal example is a single lattice
protein confined on a minimal grid, with no free sites. Finding its minimum
energy is a hard optimization problem, withsimilarities to scheduling problems.
It can be recast as a quadratic unconstrained binary optimization (QUBO)
problem amenable to classical and quantum approaches. We show that this problem
in its QUBO form can be swiftly and consistently solved for chain length 48,
using either classical simulated annealing or hybrid quantum-classical
annealing on a D-Wave system. In fact, the latter computations required about
10 seconds. We also test linear and quadratic programming methods, which work
well for a lattice gas but struggle with chain constraints. All methods are
benchmarked against exact results obtained from exhaustive structure
enumeration, at a high computational cost.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: 研究了强化学习（RL）能否帮助蛋白质语言模型（PLM）突破预训练的先验，揭示潜藏的序列-结构-功能规则。


<details>
  <summary>Details</summary>
Motivation: 探索RL是否能扩展PLM的功能，提高蛋白质设计的样本效率和成功率。

Method: 在抗菌肽设计、激酶变体优化、抗体工程和反向折叠四个领域结合RL与PLM，使用多种RL算法和模型类别。

Result: RL能显著提高成功率和样本效率，其效果取决于任务空间、奖励精确性和策略容量的相互作用。

Conclusion: RL在蛋白质设计中具有潜力，但需优先优化奖励建模和校准，并根据任务难度选择合适的算法和容量分配。

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [4] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman,Yutong Bai,Fadi Atieh,Elizabeth Weaver,Eric Qu,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 研究表明，未经修改的Transformer模型可以直接处理笛卡尔坐标，无需预定义图或物理先验，达到与GNN相当的分子能量和力预测性能。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer是否能在无需预定义图或物理先验的情况下，近似分子能量和力，以克服GNN固定感受野的限制。

Method: 使用标准Transformer模型直接处理笛卡尔坐标，对比训练计算预算下的性能与GNN。

Result: Transformer实现了与GNN相当的能量和力预测误差，并表现出物理一致的注意力权重模式。

Conclusion: 研究挑战了GNN中硬编码图的必要性，表明Transformer可以自适应学习GNN的有利特性。

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular
machine learning, particularly for molecular property prediction and machine
learning interatomic potentials (MLIPs). GNNs perform message passing on
predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor
scheme. While this design aligns with the locality present in many molecular
tasks, a hard-coded graph can limit expressivity due to the fixed receptive
field and slows down inference with sparse graph operations. In this work, we
investigate whether pure, unmodified Transformers trained directly on Cartesian
coordinates$\unicode{x2013}$without predefined graphs or physical
priors$\unicode{x2013}$can approximate molecular energies and forces. As a
starting point for our analysis, we demonstrate how to train a Transformer to
competitive energy and force mean absolute errors under a matched training
compute budget, relative to a state-of-the-art equivariant GNN on the OMol25
dataset. We discover that the Transformer learns physically consistent
patterns$\unicode{x2013}$such as attention weights that decay inversely with
interatomic distance$\unicode{x2013}$and flexibly adapts them across different
molecular environments due to the absence of hard-coded biases. The use of a
standard Transformer also unlocks predictable improvements with respect to
scaling training resources, consistent with empirical scaling laws observed in
other domains. Our results demonstrate that many favorable properties of GNNs
can emerge adaptively in Transformers, challenging the necessity of hard-coded
graph inductive biases and pointing toward standardized, scalable architectures
for molecular modeling.

</details>
