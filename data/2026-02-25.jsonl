{"id": "2602.20176", "pdf": "https://arxiv.org/pdf/2602.20176", "abs": "https://arxiv.org/abs/2602.20176", "authors": ["Ziyi Yang", "Zitong Tian", "Yinjun Jia", "Tianyi Zhang", "Jiqing Zheng", "Hao Wang", "Yubu Su", "Juncai He", "Lei Liu", "Yanyan Lan"], "title": "Cross-Chirality Generalization by Axial Vectors for Hetero-Chiral Protein-Peptide Interaction Design", "categories": ["q-bio.BM", "cs.LG"], "comment": "9 pages, 5 figures", "summary": "D-peptide binders targeting L-proteins have promising therapeutic potential. Despite rapid advances in machine learning-based target-conditioned peptide design, generating D-peptide binders remains largely unexplored. In this work, we show that by injecting axial features to $E(3)$-equivariant (polar) vector features,it is feasible to achieve cross-chirality generalization from homo-chiral (L--L) training data to hetero-chiral (D--L) design tasks. By implementing this method within a latent diffusion model, we achieved D-peptide binder design that not only outperforms existing tools in in silico benchmarks, but also demonstrates efficacy in wet-lab validation. To our knowledge, our approach represents the first wet-lab validated generative AI for the de novo design of D-peptide binders, offering new perspectives on handling chirality in protein design."}
{"id": "2602.20449", "pdf": "https://arxiv.org/pdf/2602.20449", "abs": "https://arxiv.org/abs/2602.20449", "authors": ["Anna Hart", "Chi Han", "Jeonghwan Kim", "Huimin Zhao", "Heng Ji"], "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space despite a vocabulary of only 20 amino acids. These differences motivate research into how transformer-based architectures operate differently in the protein domain and how we can better leverage PLMs to solve protein-related tasks. In this work, we begin by directly comparing how the distribution of information stored across layers of attention heads differs between the protein and natural language domain. Furthermore, we adapt a simple early-exit technique-originally used in the natural language domain to improve efficiency at the cost of performance-to achieve both increased accuracy and substantial efficiency gains in protein non-structural property prediction by allowing the model to automatically select protein representations from the intermediate layers of the PLMs for the specific task and protein at hand. We achieve performance gains ranging from 0.4 to 7.01 percentage points while simultaneously improving efficiency by over 10 percent across models and non-structural prediction tasks. Our work opens up an area of research directly comparing how language models change behavior when moved into the protein domain and advances language modeling in biological domains."}
