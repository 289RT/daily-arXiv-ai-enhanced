<div id=toc></div>

# Table of Contents

- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [1] [PFMBench: Protein Foundation Model Benchmark](https://arxiv.org/abs/2506.14796)
*Zhangyang Gao,Hao Wang,Cheng Tan,Chenrui Xu,Mengdi Liu,Bozhen Hu,Linlin Chao,Xiaoming Zhang,Stan Z. Li*

Main category: q-bio.BM

TL;DR: 该研究分析了蛋白质基础模型的研究现状和未来方向，提出了PFMBench基准，用于全面评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质基础模型研究缺乏统一的评估标准，阻碍了对模型泛化能力和局限性的深入理解。

Method: 通过构建PFMBench基准，使用17个先进模型在38个任务（涵盖8个关键领域）上进行数百次实验。

Result: PFMBench揭示了任务间的相关性，识别了表现最好的模型，并提供了简化的评估协议。

Conclusion: PFMBench填补了现有研究的空白，为蛋白质基础模型的开发与评估提供了标准化工具。

Abstract: This study investigates the current landscape and future directions of
protein foundation model research. While recent advancements have transformed
protein science and engineering, the field lacks a comprehensive benchmark for
fair evaluation and in-depth understanding. Since ESM-1B, numerous protein
foundation models have emerged, each with unique datasets and methodologies.
However, evaluations often focus on limited tasks tailored to specific models,
hindering insights into broader generalization and limitations. Specifically,
researchers struggle to understand the relationships between tasks, assess how
well current models perform across them, and determine the criteria in
developing new foundation models. To fill this gap, we present PFMBench, a
comprehensive benchmark evaluating protein foundation models across 38 tasks
spanning 8 key areas of protein science. Through hundreds of experiments on 17
state-of-the-art models across 38 tasks, PFMBench reveals the inherent
correlations between tasks, identifies top-performing models, and provides a
streamlined evaluation protocol. Code is available at
\href{https://github.com/biomap-research/PFMBench}{\textcolor{blue}{GitHub}}.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [2] [Engineering Supercomputing Platforms for Biomolecular Applications](https://arxiv.org/abs/2506.15585)
*Robert Welch,Charles Laughton,Oliver Henrich,Tom Burnley,Daniel Cole,Alan Real,Sarah Harris,James Gebbie-Rayet*

Main category: physics.bio-ph

TL;DR: 论文对多种计算生物学软件在不同HPC硬件上的性能、能效和存储需求进行了基准测试，发现没有单一硬件能优化所有任务，且新硬件支持更复杂。GPU效率高，但存储和运维挑战突出。


<details>
  <summary>Details</summary>
Motivation: 评估不同HPC硬件对计算生物学软件的适用性，以指导未来硬件选择和系统优化。

Method: 在多种HPC硬件（如AMD EPYC CPU、NVIDIA/AMD GPU）上测试GROMACS等软件的运行性能、能效及存储需求，并分析用户体验。

Result: GPU适合大多数任务，但硬件多样性是关键；新硬件兼容但运维复杂；数据存储需求高且设施不足；系统部署维护难度增加。

Conclusion: 建议通过DevOps培训、扩展联盟支持及容器化工具优化HPC系统，以应对复杂硬件环境和存储挑战。

Abstract: A range of computational biology software (GROMACS, AMBER, NAMD, LAMMPS,
OpenMM, Psi4 and RELION) was benchmarked on a representative selection of HPC
hardware, including AMD EPYC 7742 CPU nodes, NVIDIA V100 and AMD MI250X GPU
nodes, and an NVIDIA GH200 testbed. The raw performance, power efficiency and
data storage requirements of the software was evaluated for each HPC facility,
along with qualitative factors such as the user experience and software
environment. It was found that the diversity of methods used within
computational biology means that there is no single HPC hardware that can
optimally run every type of HPC job, and that diverse hardware is the only way
to properly support all methods. New hardware, such as AMD GPUs and Nvidia AI
chips, are mostly compatible with existing methods, but are also more
labour-intensive to support. GPUs offer the most efficient way to run most
computational biology tasks, though some tasks still require CPUs. A fast HPC
node running molecular dynamics can produce around 10GB of data per day,
however, most facilities and research institutions lack short-term and
long-term means to store this data. Finally, as the HPC landscape has become
more complex, deploying software and keeping HPC systems online has become more
difficult. This situation could be improved through hiring/training in DevOps
practices, expanding the consortium model to provide greater support to HPC
system administrators, and implementing build
frameworks/containerisation/virtualisation tools to allow users to configure
their own software environment, rather than relying on centralised software
installations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation](https://arxiv.org/abs/2506.15309)
*Júlia Vilalta-Mor,Alexis Molina,Laura Ortega Varga,Isaac Filella-Merce,Victor Guallar*

Main category: cs.LG

TL;DR: 提出了一种结合序列到序列变分自编码器的主动学习方法，用于优化多目标药物分子设计。


<details>
  <summary>Details</summary>
Motivation: 药物发现中同时优化多个治疗靶点面临稀疏奖励和设计约束冲突的挑战。

Method: 采用结构化主动学习范式，交替扩展化学可行区域并逐步约束分子以满足多目标要求。

Result: 在针对三种冠状病毒蛋白酶的概念验证中，成功生成了多样化的泛抑制剂候选分子。

Conclusion: 该框架为复杂多靶点药物设计提供了可推广的解决路径。

Abstract: Simultaneously optimizing molecules against multiple therapeutic targets
remains a profound challenge in drug discovery, particularly due to sparse
rewards and conflicting design constraints. We propose a structured active
learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational
autoencoder (VAE) into iterative loops designed to balance chemical diversity,
molecular quality, and multi-target affinity. Our method alternates between
expanding chemically feasible regions of latent space and progressively
constraining molecules based on increasingly stringent multi-target docking
thresholds. In a proof-of-concept study targeting three related coronavirus
main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently
generated a structurally diverse set of pan-inhibitor candidates. We
demonstrate that careful timing and strategic placement of chemical filters
within this active learning pipeline markedly enhance exploration of beneficial
chemical space, transforming the sparse-reward, multi-objective drug design
problem into an accessible computational task. Our framework thus provides a
generalizable roadmap for efficiently navigating complex polypharmacological
landscapes.

</details>
