<div id=toc></div>

# Table of Contents

- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [1] [BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning](https://arxiv.org/abs/2510.01632)
*Xin Wang,Carlos Oliver*

Main category: q-bio.BM

TL;DR: BioBlobs是一种可插拔的模块，通过动态分区蛋白质结构为灵活大小的非重叠子结构（“blobs”），并将其量化为共享且可解释的代码本，从而提升蛋白质表示学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质表示学习模型（PRL）依赖于固定的子结构（如k-hop和固定半径邻域），这会扭曲蛋白质功能相关的信号。BioBlobs旨在更准确地捕捉功能相关的蛋白质子结构。

Method: BioBlobs动态地将蛋白质结构分区为灵活大小的非重叠子结构（“blobs”），并将其量化为共享代码本，生成功能相关的蛋白质子结构离散词汇表。

Result: BioBlobs显著提升了广泛使用的蛋白质编码器（如GVP-GNN）的性能，并在多个PRL任务中表现出色。

Conclusion: BioBlobs通过直接捕捉功能相关的蛋白质子结构，不仅提高了预测性能，还为蛋白质功能提供了机制性见解。

Abstract: Protein function is driven by coherent substructures which vary in size and
topology, yet current protein representation learning models (PRL) distort
these signals by relying on rigid substructures such as k-hop and fixed radius
neighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable
module that represents proteins by dynamically partitioning structures into
flexibly-sized, non-overlapping substructures ("blobs"). The resulting blobs
are quantized into a shared and interpretable codebook, yielding a discrete
vocabulary of function-relevant protein substructures used to compute protein
embeddings. We show that BioBlobs representations improve the performance of
widely used protein encoders such as GVP-GNN across various PRL tasks. Our
approach highlights the value of architectures that directly capture
function-relevant protein substructures, enabling both improved predictive
performance and mechanistic insight into protein function.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: 该研究探讨了强化学习（RL）是否能通过与蛋白质语言模型（PLMs）结合，超越监督学习的限制，揭示序列-结构-功能的潜在规律。


<details>
  <summary>Details</summary>
Motivation: 研究者关注RL是否能扩展PLMs的能力，特别是在蛋白质设计中探索新的序列-结构-功能关系。

Method: 研究通过结合RL和PLMs，在抗菌肽设计、激酶变体优化、抗体工程和逆向折叠四个领域进行了实验，使用了多种RL算法和模型类别。

Result: 结果显示，RL显著提高了成功率和采样效率，但效果取决于任务上限、奖励准确性和政策容量的相互作用。

Conclusion: 研究为蛋白质设计中RL的应用提供了实用建议：优先建模和校准奖励，并根据任务难度匹配算法和正则化强度。

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [3] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman,Yutong Bai,Fadi Atieh,Elizabeth Weaver,Eric Qu,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 本文探讨了纯Transformer模型在分子能量和力预测中的应用，发现其表现可与图神经网络（GNN）媲美，且无需预设图结构，具有自适应性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究背景是GNN在分子机器学习中的局限性，如固定的感受野和稀疏图操作影响推理速度。本文旨在探索Transformer是否能在无预设图的条件下学习分子能量和力。

Method: 方法是用标准Transformer直接训练于笛卡尔坐标上，无预设图或物理先验，并在OMol25数据集上评估其性能。

Result: 结果表明，Transformer不仅能达到与GNN相当的误差水平，还能自适应学习物理规律（如注意力权重与距离成反比），且训练扩展性良好。

Conclusion: 结论是Transformer的自适应性挑战了传统GNN的必要性，为分子建模提供了更标准化、可扩展的架构选择。

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular
machine learning, particularly for molecular property prediction and machine
learning interatomic potentials (MLIPs). GNNs perform message passing on
predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor
scheme. While this design aligns with the locality present in many molecular
tasks, a hard-coded graph can limit expressivity due to the fixed receptive
field and slows down inference with sparse graph operations. In this work, we
investigate whether pure, unmodified Transformers trained directly on Cartesian
coordinates$\unicode{x2013}$without predefined graphs or physical
priors$\unicode{x2013}$can approximate molecular energies and forces. As a
starting point for our analysis, we demonstrate how to train a Transformer to
competitive energy and force mean absolute errors under a matched training
compute budget, relative to a state-of-the-art equivariant GNN on the OMol25
dataset. We discover that the Transformer learns physically consistent
patterns$\unicode{x2013}$such as attention weights that decay inversely with
interatomic distance$\unicode{x2013}$and flexibly adapts them across different
molecular environments due to the absence of hard-coded biases. The use of a
standard Transformer also unlocks predictable improvements with respect to
scaling training resources, consistent with empirical scaling laws observed in
other domains. Our results demonstrate that many favorable properties of GNNs
can emerge adaptively in Transformers, challenging the necessity of hard-coded
graph inductive biases and pointing toward standardized, scalable architectures
for molecular modeling.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [4] [Folding lattice proteins confined on minimal grids using a quantum-inspired encoding](https://arxiv.org/abs/2510.01890)
*Anders Irbäck,Lucas Knuthson,Sandipan Mohanty*

Main category: quant-ph

TL;DR: 本研究探讨了在密集蛋白质系统中使用传统显式链方法时遇到的位阻问题，并通过将其转化为QUBO问题，展示了古典模拟退火和混合量子-古典退火方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究位阻问题在密集蛋白质系统中的挑战，探索高效的优化方法。

Method: 将问题转化为QUBO形式，使用古典模拟退火、混合量子-古典退火、线性及二次规划方法进行求解。

Result: 古典模拟退火和混合量子-古典退火能快速解决链长为48的问题，后者仅需约10秒。线性及二次规划方法在某些情况下效果不佳。

Conclusion: QUBO形式的优化方法为解决密集蛋白质系统中的位阻问题提供了高效途径，尤其量子-古典混合方法表现出色。

Abstract: Steric clashes pose a challenge when exploring dense protein systems using
conventional explicit-chain methods. A minimal example is a single lattice
protein confined on a minimal grid, with no free sites. Finding its minimum
energy is a hard optimization problem, withsimilarities to scheduling problems.
It can be recast as a quadratic unconstrained binary optimization (QUBO)
problem amenable to classical and quantum approaches. We show that this problem
in its QUBO form can be swiftly and consistently solved for chain length 48,
using either classical simulated annealing or hybrid quantum-classical
annealing on a D-Wave system. In fact, the latter computations required about
10 seconds. We also test linear and quadratic programming methods, which work
well for a lattice gas but struggle with chain constraints. All methods are
benchmarked against exact results obtained from exhaustive structure
enumeration, at a high computational cost.

</details>
