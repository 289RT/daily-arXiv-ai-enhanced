<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 2]
- [stat.ML](#stat.ML) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction](https://arxiv.org/abs/2510.09668)
*Maryam Abdollahi Shamami,Babak Teimourpour,Farshad Sharifi*

Main category: cs.LG

TL;DR: 提出一种结合分子嵌入和临床评分的可解释高效框架来预测药物相互作用，并通过优化策略实现高性能。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用是导致不良反应的主要原因，了解药物是否相互作用对安全用药至关重要。

Method: 结合Mol2Vec和SMILES-BERT分子嵌入，引入无泄漏的临床评分RBScore，并采用三阶段优化策略RSmpl-ACO-PSO训练分类器。

Result: 在真实数据集上表现出高预测准确性（ROC-AUC 0.911），在糖尿病队列中泛化能力强。

Conclusion: 该框架为构建可靠且可解释的药物相互作用模型提供了实用途径。

Abstract: Drug-drug interactions (DDIs) are a leading cause of preventable adverse
events, often complicating treatment and increasing healthcare costs. At the
same time, knowing which drugs do not interact is equally important, as such
knowledge supports safer prescriptions and better patient outcomes. In this
study, we propose an interpretable and efficient framework that blends modern
machine learning with domain knowledge to improve DDI prediction. Our approach
combines two complementary molecular embeddings - Mol2Vec, which captures
fragment-level structural patterns, and SMILES-BERT, which learns contextual
chemical features - together with a leakage-free, rule-based clinical score
(RBScore) that injects pharmacological knowledge without relying on interaction
labels. A lightweight neural classifier is then optimized using a novel
three-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global
exploration and local refinement for stable performance. Experiments on
real-world datasets demonstrate that the model achieves high predictive
accuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a
clinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,
studies show how embedding fusion, RBScore, and the optimizer each contribute
to precision and robustness. Together, these results highlight a practical
pathway for building reliable, interpretable, and computationally efficient
models that can support safer drug therapies and clinical decision-making.

</details>


### [2] [Protein as a Second Language for LLMs](https://arxiv.org/abs/2510.11188)
*Xinhui Chen,Zuchao Li,Mengqi Gao,Yufeng Zhang,Chak Tou Leong,Haoyang Li,Jiaqi Chen*

Main category: cs.LG

TL;DR: 介绍了一种名为'Protein-as-Second-Language'的框架，将蛋白质序列转化为语言模型可理解的句子，通过零样本任务实现蛋白质功能预测，性能优于专用模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖任务特定适配器或大规模监督微调的问题，探索通用语言模型在蛋白质功能预测中的应用。

Method: 基于序列-问题-答案三元组，构建双语语料库，利用大型语言模型进行零样本预测。

Result: 在多种LLM和GPT-4上表现优异，平均提升7%（最高17.2%），超越专用模型。

Conclusion: 通用语言模型通过蛋白质语言线索可超越专用模型，为蛋白质理解提供可扩展途径。

Abstract: Deciphering the function of unseen protein sequences is a fundamental
challenge with broad scientific impact, yet most existing methods depend on
task-specific adapters or large-scale supervised fine-tuning. We introduce the
"Protein-as-Second-Language" framework, which reformulates amino-acid sequences
as sentences in a novel symbolic language that large language models can
interpret through contextual exemplars. Our approach adaptively constructs
sequence-question-answer triples that reveal functional cues in a zero-shot
setting, without any further training. To support this process, we curate a
bilingual corpus of 79,926 protein-QA instances spanning attribute prediction,
descriptive understanding, and extended reasoning. Empirically, our method
delivers consistent gains across diverse open-source LLMs and GPT-4, achieving
up to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned
protein-specific language models. These results highlight that generic LLMs,
when guided with protein-as-language cues, can outperform domain-specialized
models, offering a scalable pathway for protein understanding in foundation
models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [3] [Calibrating Generative Models](https://arxiv.org/abs/2510.10020)
*Henry D. Smith,Nathaniel L. Diamant,Brian L. Trippe*

Main category: stat.ML

TL;DR: 生成模型常面临校准问题，本文通过约束优化和两种替代目标（relax loss和reward loss）显著降低校准误差。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中概率统计与期望值偏离的校准问题。

Method: 提出基于Kullback-Leibler散度的约束优化，并引入relax loss和reward loss两种替代目标进行微调。

Result: 在蛋白质设计、图像生成和语言模型等应用中，大幅降低数百个同时约束和十亿参数模型的校准误差。

Conclusion: 两种替代目标有效提升生成模型的校准性，适用于多种大规模应用场景。

Abstract: Generative models frequently suffer miscalibration, wherein class
probabilities and other statistics of the sampling distribution deviate from
desired values. We frame calibration as a constrained optimization problem and
seek the closest model in Kullback-Leibler divergence satisfying calibration
constraints. To address the intractability of imposing these constraints
exactly, we introduce two surrogate objectives for fine-tuning: (1) the relax
loss, which replaces the constraint with a miscalibration penalty, and (2) the
reward loss, which converts calibration into a reward fine-tuning problem. We
demonstrate that these approaches substantially reduce calibration error across
hundreds of simultaneous constraints and models with up to one billion
parameters, spanning applications in protein design, image generation, and
language modeling.

</details>
