<div id=toc></div>

# Table of Contents

- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [1] [CryoNet.Refine: A One-step Diffusion Model for Rapid Refinement of Structural Models with Cryo-EM Density Map Restraints](https://arxiv.org/abs/2602.22263)
*Fuyao Huang,Xiaozhu Yu,Kui Xu,Qiangfeng Cliff Zhang*

Main category: q-bio.BM

TL;DR: CryoNet.Refine是一种基于深度学习的端到端框架，用于自动化加速分子结构的冷冻电镜细化，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统冷冻电镜细化方法（如Phenix.real_space_refine）计算成本高、需要手动调整的问题。

Method: 采用一步扩散模型，结合密度感知损失函数和立体化学约束，快速优化结构以适应实验数据。

Result: 在模型-密度图相关性和几何质量指标上优于传统方法。

Conclusion: CryoNet.Refine为下一代冷冻电镜结构细化提供了可扩展、自动化的强大工具。

Abstract: High-resolution structure determination by cryo-electron microscopy (cryo-EM) requires the accurate fitting of an atomic model into an experimental density map. Traditional refinement pipelines such as Phenix.real_space_refine and Rosetta are computationally expensive, demand extensive manual tuning, and present a significant bottleneck for researchers. We present CryoNet.Refine, an end-to-end deep learning framework that automates and accelerates molecular structure refinement. Our approach utilizes a one-step diffusion model that integrates a density-aware loss function with robust stereochemical restraints, enabling rapid optimization of a structure against experimental data. CryoNet.Refine provides a unified and versatile solution capable of refining protein complexes as well as DNA/RNA-protein complexes. In benchmarks against Phenix.real_space_refine, CryoNet.Refine consistently achieves substantial improvements in both model-map correlation and overall geometric quality metrics. By offering a scalable, automated, and powerful alternative, CryoNet.Refine aims to serve as an essential tool for next-generation cryo-EM structure refinement. Web server: https://cryonet.ai/refine; Source code: https://github.com/kuixu/cryonet.refine.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models](https://arxiv.org/abs/2602.23179)
*Gal Kesten-Pomeranz,Yaniv Nikankin,Anja Reusch,Tomer Tsaban,Ora Schueler-Furman,Yonatan Belinkov*

Main category: cs.LG

TL;DR: 研究了蛋白质语言模型（PLMs）如何检测精确和近似重复序列的内部机制，揭示了其通过结合语言模式匹配和生物学知识的方法。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列中的重复片段对结构和功能至关重要，研究PLMs如何识别这些重复有助于理解其内部机制。

Method: 通过分析PLMs在掩码标记预测中的行为，揭示其特征表示构建和注意力机制。

Result: PLMs通过通用位置注意力头和生物学专用组件构建特征表示，并通过诱导头关注重复片段的对齐标记。

Conclusion: PLMs结合语言模式匹配和生物学知识解决重复识别任务，为研究更复杂进化过程奠定了基础。

Abstract: Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.

</details>
