{"id": "2506.19532", "pdf": "https://arxiv.org/pdf/2506.19532", "abs": "https://arxiv.org/abs/2506.19532", "authors": ["Andrea Hunklinger", "Noelia Ferruz"], "title": "Toward the Explainability of Protein Language Models for Sequence Design", "categories": ["q-bio.BM"], "comment": "12 pages, 6 figures", "summary": "Transformer-based language models excel in a variety of protein-science tasks\nthat range from structure prediction to the design of functional enzymes.\nHowever, these models operate as black boxes, and their underlying working\nprinciples remain unclear. Here, we survey emerging applications of explainable\nartificial intelligence (XAI) to protein language models (pLMs) and describe\ntheir potential in protein research. We break down the workflow of a generative\ndecoder-only Transformer into four information contexts: (i) training\nsequences, (ii) input prompt, (iii) model architecture, and (iv) output\nsequence. For each, we describe existing methods and applications of XAI.\nAdditionally, from published studies we distil five (potential) roles that XAI\ncan play in protein design: Evaluator, Multitasker, Engineer, Coach, and\nTeacher, with the Evaluator role being the only one widely adopted so far.\nThese roles aim to help both protein science practitioners and model developers\nunderstand the possibilities and limitations of implementing XAI for the design\nof sequences. Finally, we highlight the critical areas of application for the\nfuture, including risks related to security, trustworthiness, and bias, and we\ncall for community benchmarks, open-source tooling, and domain-specific\nvisualizations to advance explainable protein design. Overall, our analysis\naims to move the discussion toward the use of XAI in protein design."}
{"id": "2506.19820", "pdf": "https://arxiv.org/pdf/2506.19820", "abs": "https://arxiv.org/abs/2506.19820", "authors": ["Felix Faltings", "Hannes Stark", "Regina Barzilay", "Tommi Jaakkola"], "title": "ProxelGen: Generating Proteins as 3D Densities", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "We develop ProxelGen, a protein structure generative model that operates on\n3D densities as opposed to the prevailing 3D point cloud representations.\nRepresenting proteins as voxelized densities, or proxels, enables new tasks and\nconditioning capabilities. We generate proteins encoded as proxels via a 3D\nCNN-based VAE in conjunction with a diffusion model operating on its latent\nspace. Compared to state-of-the-art models, ProxelGen's samples achieve higher\nnovelty, better FID scores, and the same level of designability as the training\nset. ProxelGen's advantages are demonstrated in a standard motif scaffolding\nbenchmark, and we show how 3D density-based generation allows for more flexible\nshape conditioning."}
{"id": "2506.19834", "pdf": "https://arxiv.org/pdf/2506.19834", "abs": "https://arxiv.org/abs/2506.19834", "authors": ["Viatcheslav Gurev", "Timothy Rumbell"], "title": "A standard transformer and attention with linear biases for molecular conformer generation", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": "Revision of paper at OpenReview:\n  https://openreview.net/forum?id=BjjerMYL3F", "summary": "Sampling low-energy molecular conformations, spatial arrangements of atoms in\na molecule, is a critical task for many different calculations performed in the\ndrug discovery and optimization process. Numerous specialized equivariant\nnetworks have been designed to generate molecular conformations from 2D\nmolecular graphs. Recently, non-equivariant transformer models have emerged as\na viable alternative due to their capability to scale to improve\ngeneralization. However, the concern has been that non-equivariant models\nrequire a large model size to compensate the lack of equivariant bias. In this\npaper, we demonstrate that a well-chosen positional encoding effectively\naddresses these size limitations. A standard transformer model incorporating\nrelative positional encoding for molecular graphs when scaled to 25 million\nparameters surpasses the current state-of-the-art non-equivariant base model\nwith 64 million parameters on the GEOM-DRUGS benchmark. We implemented relative\npositional encoding as a negative attention bias that linearly increases with\nthe shortest path distances between graph nodes at varying slopes for different\nattention heads, similar to ALiBi, a widely adopted relative positional\nencoding technique in the NLP domain. This architecture has the potential to\nserve as a foundation for a novel class of generative models for molecular\nconformations."}
{"id": "2506.19841", "pdf": "https://arxiv.org/pdf/2506.19841", "abs": "https://arxiv.org/abs/2506.19841", "authors": ["Adittya Pal"], "title": "Thermodynamic free energy map for the non-oxidative glycolysis pathways", "categories": ["q-bio.MN", "cs.SY", "eess.SY", "q-bio.BM"], "comment": "10 figures, 6 tables", "summary": "Designing reaction pathways that maximize the production of a target compound\nin a given metabolic network is a fundamental problem in systems biology. In\nthis study, we systematically explore the non-oxidative glycolysis metabolic\nnetwork, guided by the principle that reactions with negative Gibbs free energy\ndifferences are thermodynamically favored. We enumerate alternative pathways\nthat implement the net non-oxidative glycolysis reaction, categorized by their\nlength. Our analysis reveals several alternative thermodynamically favorable\npathways beyond those reported in experiments. In addition, we identify\nmolecules within the network, such as 3-hydroxypropionic acid, that may have\nsignificant potential for further investigation."}
