<div id=toc></div>

# Table of Contents

- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [1] [DL4Proteins Jupyter Notebooks Teach how to use Artificial Intelligence for Biomolecular Structure Prediction and Design](https://arxiv.org/abs/2511.02128)
*Michael Chungyoun,Gabe Au,Britnie Carpentier,Sreevarsha Puvada,Courtney Thomas,Jeffrey J. Gray*

Main category: q-bio.BM

TL;DR: DL4Proteins是一个包含十个交互式笔记本模块的系列，旨在提高蛋白质科学领域的AI素养，从基础机器学习概念到前沿蛋白质结构预测与设计。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在蛋白质科学中的影响日益增长，需要针对现有科学家和新研究者提供增强的教育材料，以提高他们的AI能力。

Method: 开发了DL4Proteins系列，通过交互式笔记本模块介绍机器学习概念、训练ML模型，并展示蛋白质结构预测和设计的最新工具。

Result: 学习者仅需浏览器即可访问专业蛋白质工程师使用的先进计算工具，从而扩大了AI驱动蛋白质研究的参与范围。

Conclusion: DL4Proteins通过提高AI工具的普及性，推动了蛋白质科学的广泛参与与发展。

Abstract: Computational methods for predicting and designing biomolecular structures
are increasingly powerful. While previous approaches relied on physics-based
modeling, modern tools, such as AlphaFold2 in CASP14, leverage artificial
intelligence (AI) to achieve significantly improved performance. The growing
impact of AI-based tools in protein science necessitates enhanced educational
materials that improve AI literacy among both established scientists seeking to
deepen their expertise and new researchers entering the field. To address this
need, we developed DL4Proteins, a series of ten interactive notebook modules
that introduce fundamental machine learning (ML) concepts, guide users through
training ML models for protein-related tasks, and ultimately present
cutting-edge protein structure prediction and design pipelines. With nothing
more than a web browser, learners can now access state-of-the-art computational
tools employed by professional protein engineers - ranging from all-atom
protein design to fine-tuning protein language models for biophysically
relevant functional tasks. By increasing accessibility, this notebook series
broadens participation in AI-driven protein research. The complete notebook
series is publicly available at
https://github.com/Graylab/DL4Proteins-notebooks.

</details>


### [2] [Machine Learning for RNA Secondary Structure Prediction: a review of current methods and challenges](https://arxiv.org/abs/2511.02622)
*Giuseppe Sacco,Giovanni Bussi,Guido Sanguinetti*

Main category: q-bio.BM

TL;DR: 综述了RNA二级结构预测领域的最新进展，从传统热力学方法转向数据驱动的机器学习和深度学习方法，讨论了模型的泛化危机及应对策略，并展望了未来挑战。


<details>
  <summary>Details</summary>
Motivation: RNA二级结构预测是计算生物学的核心问题，对理解分子功能和设计新型疗法至关重要。

Method: 综述了单序列、基于进化和混合（机器学习与生物物理学结合）模型，强调通过RNA基础模型解决数据稀缺问题。

Result: 现有模型在新RNA家族上表现不佳（泛化危机），促使采用更严格的同源感知基准测试。

Conclusion: 未来需解决复杂模体预测、长序列扩展、修饰核苷酸多样性等问题，并呼吁建立标准化前瞻性基准系统。

Abstract: Predicting the secondary structure of RNA is a core challenge in
computational biology, essential for understanding molecular function and
designing novel therapeutics. The field has evolved from foundational but
accuracy-limited thermodynamic approaches to a new data-driven paradigm
dominated by machine learning and deep learning. These models learn folding
patterns directly from data, leading to significant performance gains. This
review surveys the modern landscape of these methods, covering single-sequence,
evolutionary-based, and hybrid models that blend machine learning with
biophysics. A central theme is the field's "generalization crisis," where
powerful models were found to fail on new RNA families, prompting a
community-wide shift to stricter, homology-aware benchmarking. In response to
the underlying challenge of data scarcity, RNA foundation models have emerged,
learning from massive, unlabeled sequence corpora to improve generalization.
Finally, we look ahead to the next set of major hurdles-including the accurate
prediction of complex motifs like pseudoknots, scaling to kilobase-length
transcripts, incorporating the chemical diversity of modified nucleotides, and
shifting the prediction target from static structures to the dynamic ensembles
that better capture biological function. We also highlight the need for a
standardized, prospective benchmarking system to ensure unbiased validation and
accelerate progress.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation](https://arxiv.org/abs/2511.02769)
*Bum Chul Kwon,Ben Shapira,Moshiko Raboh,Shreyans Sethi,Shruti Murarka,Joseph A Morrone,Jianying Hu,Parthasarathy Suryanarayanan*

Main category: cs.LG

TL;DR: 论文提出了一种名为STAR-VAE的生成模型，用于生成药物分子，通过Transformer结构和SELFIES编码保证有效性，支持条件生成和快速微调。


<details>
  <summary>Details</summary>
Motivation: 药物分子的化学空间庞大，需要开发能够学习广泛化学分布、支持条件生成并提供快速分子生成的模型。

Method: 采用Transformer编码器和自回归解码器的变分自编码器(STAR-VAE)，基于SELFIES编码保证语法有效性，并通过低秩适配器(LoRA)实现高效微调。

Result: 在GuacaMol和MOSES基准测试中表现优异，支持无条件探索和属性感知生成，并在Tartarus基准中提升结合预测得分。

Conclusion: 结合结构化条件和参数高效微调的现代VAE在分子生成任务中仍具竞争力。

Abstract: The chemical space of drug-like molecules is vast, motivating the development
of generative models that must learn broad chemical distributions, enable
conditional generation by capturing structure-property representations, and
provide fast molecular generation. Meeting the objectives depends on modeling
choices, including the probabilistic modeling approach, the conditional
generative formulation, the architecture, and the molecular input
representation. To address the challenges, we present STAR-VAE
(Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder),
a scalable latent-variable framework with a Transformer encoder and an
autoregressive Transformer decoder. It is trained on 79 million drug-like
molecules from PubChem, using SELFIES to guarantee syntactic validity. The
latent-variable formulation enables conditional generation: a property
predictor supplies a conditioning signal that is applied consistently to the
latent prior, the inference network, and the decoder. Our contributions are:
(i) a Transformer-based latent-variable encoder-decoder model trained on
SELFIES representations; (ii) a principled conditional latent-variable
formulation for property-guided generation; and (iii) efficient finetuning with
low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation
with limited property and activity data. On the GuacaMol and MOSES benchmarks,
our approach matches or exceeds baselines, and latent-space analyses reveal
smooth, semantically structured representations that support both unconditional
exploration and property-aware generation. On the Tartarus benchmarks, the
conditional model shifts docking-score distributions toward stronger predicted
binding. These results suggest that a modernized, scale-appropriate VAE remains
competitive for molecular generation when paired with principled conditioning
and parameter-efficient finetuning.

</details>
