{"id": "2510.01632", "pdf": "https://arxiv.org/pdf/2510.01632", "abs": "https://arxiv.org/abs/2510.01632", "authors": ["Xin Wang", "Carlos Oliver"], "title": "BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Protein function is driven by coherent substructures which vary in size and\ntopology, yet current protein representation learning models (PRL) distort\nthese signals by relying on rigid substructures such as k-hop and fixed radius\nneighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable\nmodule that represents proteins by dynamically partitioning structures into\nflexibly-sized, non-overlapping substructures (\"blobs\"). The resulting blobs\nare quantized into a shared and interpretable codebook, yielding a discrete\nvocabulary of function-relevant protein substructures used to compute protein\nembeddings. We show that BioBlobs representations improve the performance of\nwidely used protein encoders such as GVP-GNN across various PRL tasks. Our\napproach highlights the value of architectures that directly capture\nfunction-relevant protein substructures, enabling both improved predictive\nperformance and mechanistic insight into protein function.", "AI": {"tldr": "BioBlobs\u662f\u4e00\u79cd\u52a8\u6001\u5206\u533a\u86cb\u767d\u8d28\u7ed3\u6784\u4e3a\u7075\u6d3b\u5927\u5c0f\u7684\u5b50\u7ed3\u6784\u7684\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u86cb\u767d\u8d28\u7f16\u7801\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u86cb\u767d\u8d28\u8868\u793a\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u521a\u6027\u5b50\u7ed3\u6784\uff0c\u626d\u66f2\u4e86\u529f\u80fd\u76f8\u5173\u7684\u4fe1\u53f7\u3002", "method": "BioBlobs\u52a8\u6001\u5206\u533a\u86cb\u767d\u8d28\u7ed3\u6784\u4e3a\u975e\u91cd\u53e0\u5b50\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u4ee3\u7801\u7c3f\u91cf\u5316\u3002", "result": "BioBlobs\u63d0\u5347\u4e86\u591a\u79cd\u86cb\u767d\u8d28\u8868\u793a\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u76f4\u63a5\u6355\u83b7\u529f\u80fd\u76f8\u5173\u5b50\u7ed3\u6784\u7684\u67b6\u6784\u80fd\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u5e76\u63d0\u4f9b\u86cb\u767d\u8d28\u529f\u80fd\u7684\u673a\u5236\u6d1e\u5bdf\u3002"}}
{"id": "2510.01571", "pdf": "https://arxiv.org/pdf/2510.01571", "abs": "https://arxiv.org/abs/2510.01571", "authors": ["Hanqun Cao", "Hongrui Zhang", "Junde Xu", "Zhou Zhang", "Lingdong Shen", "Minghao Sun", "Ge Liu", "Jinbo Xu", "Wu-Jun Li", "Jinren Ni", "Cesar de la Fuente-Nunez", "Tianfan Fu", "Yejin Choi", "Pheng-Ann Heng", "Fang Wu"], "title": "From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "24 pages, 7 figures, 4 tables", "summary": "Protein language models (PLMs) have advanced computational protein science\nthrough large-scale pretraining and scalable architectures. In parallel,\nreinforcement learning (RL) has broadened exploration and enabled precise\nmulti-objective optimization in protein design. Yet whether RL can push PLMs\nbeyond their pretraining priors to uncover latent sequence-structure-function\nrules remains unclear. We address this by pairing RL with PLMs across four\ndomains: antimicrobial peptide design, kinase variant optimization, antibody\nengineering, and inverse folding. Using diverse RL algorithms and model\nclasses, we ask if RL improves sampling efficiency and, more importantly, if it\nreveals capabilities not captured by supervised learning. Across benchmarks, RL\nconsistently boosts success rates and sample efficiency. Performance follows a\nthree-factor interaction: task headroom, reward fidelity, and policy capacity\njointly determine gains. When rewards are accurate and informative, policies\nhave sufficient capacity, and tasks leave room beyond supervised baselines,\nimprovements scale; when rewards are noisy or capacity is constrained, gains\nsaturate despite exploration. This view yields practical guidance for RL in\nprotein design: prioritize reward modeling and calibration before scaling\npolicy size, match algorithm and regularization strength to task difficulty,\nand allocate capacity where marginal gains are largest. Implementation is\navailable at https://github.com/chq1155/RL-PLM.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u80fd\u5426\u5e2e\u52a9\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u7a81\u7834\u9884\u8bad\u7ec3\u7684\u5148\u9a8c\uff0c\u63ed\u793a\u6f5c\u85cf\u7684\u5e8f\u5217-\u7ed3\u6784-\u529f\u80fd\u89c4\u5219\u3002", "motivation": "\u63a2\u7d22RL\u662f\u5426\u80fd\u6269\u5c55PLM\u7684\u529f\u80fd\uff0c\u63d0\u9ad8\u86cb\u767d\u8d28\u8bbe\u8ba1\u7684\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "method": "\u5728\u6297\u83cc\u80bd\u8bbe\u8ba1\u3001\u6fc0\u9176\u53d8\u4f53\u4f18\u5316\u3001\u6297\u4f53\u5de5\u7a0b\u548c\u53cd\u5411\u6298\u53e0\u56db\u4e2a\u9886\u57df\u7ed3\u5408RL\u4e0ePLM\uff0c\u4f7f\u7528\u591a\u79cdRL\u7b97\u6cd5\u548c\u6a21\u578b\u7c7b\u522b\u3002", "result": "RL\u80fd\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u548c\u6837\u672c\u6548\u7387\uff0c\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u4efb\u52a1\u7a7a\u95f4\u3001\u5956\u52b1\u7cbe\u786e\u6027\u548c\u7b56\u7565\u5bb9\u91cf\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "RL\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u4f18\u5148\u4f18\u5316\u5956\u52b1\u5efa\u6a21\u548c\u6821\u51c6\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\u548c\u5bb9\u91cf\u5206\u914d\u3002"}}
{"id": "2510.01890", "pdf": "https://arxiv.org/pdf/2510.01890", "abs": "https://arxiv.org/abs/2510.01890", "authors": ["Anders Irb\u00e4ck", "Lucas Knuthson", "Sandipan Mohanty"], "title": "Folding lattice proteins confined on minimal grids using a quantum-inspired encoding", "categories": ["quant-ph", "cond-mat.soft", "physics.bio-ph", "q-bio.BM"], "comment": "22 pages, 5 figures", "summary": "Steric clashes pose a challenge when exploring dense protein systems using\nconventional explicit-chain methods. A minimal example is a single lattice\nprotein confined on a minimal grid, with no free sites. Finding its minimum\nenergy is a hard optimization problem, withsimilarities to scheduling problems.\nIt can be recast as a quadratic unconstrained binary optimization (QUBO)\nproblem amenable to classical and quantum approaches. We show that this problem\nin its QUBO form can be swiftly and consistently solved for chain length 48,\nusing either classical simulated annealing or hybrid quantum-classical\nannealing on a D-Wave system. In fact, the latter computations required about\n10 seconds. We also test linear and quadratic programming methods, which work\nwell for a lattice gas but struggle with chain constraints. All methods are\nbenchmarked against exact results obtained from exhaustive structure\nenumeration, at a high computational cost.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u5bc6\u96c6\u86cb\u767d\u8d28\u7cfb\u7edf\u4e2d\u89e3\u51b3\u4f4d\u963b\u78b0\u649e\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u6700\u5c0f\u80fd\u91cf\u5bfb\u627e\u95ee\u9898\u8f6c\u5316\u4e3aQUBO\u5f62\u5f0f\uff0c\u5c55\u793a\u4e86\u7ecf\u5178\u6a21\u62df\u9000\u706b\u548c\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u9000\u706b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u663e\u5f0f\u94fe\u65b9\u6cd5\u5728\u63a2\u7d22\u5bc6\u96c6\u86cb\u767d\u8d28\u7cfb\u7edf\u65f6\u9762\u4e34\u4f4d\u963b\u78b0\u649e\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u6700\u5c0f\u80fd\u91cf\u5bfb\u627e\u95ee\u9898\u8f6c\u5316\u4e3aQUBO\u5f62\u5f0f\uff0c\u5e76\u5206\u522b\u91c7\u7528\u7ecf\u5178\u6a21\u62df\u9000\u706b\u3001\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u9000\u706b\u3001\u4ee5\u53ca\u7ebf\u6027\u4e0e\u4e8c\u6b21\u89c4\u5212\u65b9\u6cd5\u8fdb\u884c\u6c42\u89e3\u3002", "result": "\u7ecf\u5178\u6a21\u62df\u9000\u706b\u548c\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u9000\u706b\u65b9\u6cd5\u5728\u94fe\u957f\u4e3a48\u65f6\u80fd\u591f\u5feb\u901f\u4e14\u4e00\u81f4\u5730\u89e3\u51b3\u95ee\u9898\uff0c\u800c\u7ebf\u6027\u4e0e\u4e8c\u6b21\u89c4\u5212\u65b9\u6cd5\u5728\u94fe\u7ea6\u675f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "QUBO\u5f62\u5f0f\u7684\u4f18\u5316\u95ee\u9898\u9002\u5408\u901a\u8fc7\u7ecf\u5178\u548c\u91cf\u5b50\u9000\u706b\u65b9\u6cd5\u9ad8\u6548\u6c42\u89e3\uff0c\u4e3a\u5bc6\u96c6\u86cb\u767d\u8d28\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.02259", "pdf": "https://arxiv.org/pdf/2510.02259", "abs": "https://arxiv.org/abs/2510.02259", "authors": ["Tobias Kreiman", "Yutong Bai", "Fadi Atieh", "Elizabeth Weaver", "Eric Qu", "Aditi S. Krishnapriyan"], "title": "Transformers Discover Molecular Structure Without Graph Priors", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "q-bio.BM"], "comment": null, "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinates$\\unicode{x2013}$without predefined graphs or physical\npriors$\\unicode{x2013}$can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatterns$\\unicode{x2013}$such as attention weights that decay inversely with\ninteratomic distance$\\unicode{x2013}$and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u672a\u7ecf\u4fee\u6539\u7684Transformer\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u7b1b\u5361\u5c14\u5750\u6807\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u56fe\u6216\u7269\u7406\u5148\u9a8c\uff0c\u8fbe\u5230\u4e0eGNN\u76f8\u5f53\u7684\u5206\u5b50\u80fd\u91cf\u548c\u529b\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8Transformer\u662f\u5426\u80fd\u5728\u65e0\u9700\u9884\u5b9a\u4e49\u56fe\u6216\u7269\u7406\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd1\u4f3c\u5206\u5b50\u80fd\u91cf\u548c\u529b\uff0c\u4ee5\u514b\u670dGNN\u56fa\u5b9a\u611f\u53d7\u91ce\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u6807\u51c6Transformer\u6a21\u578b\u76f4\u63a5\u5904\u7406\u7b1b\u5361\u5c14\u5750\u6807\uff0c\u5bf9\u6bd4\u8bad\u7ec3\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u6027\u80fd\u4e0eGNN\u3002", "result": "Transformer\u5b9e\u73b0\u4e86\u4e0eGNN\u76f8\u5f53\u7684\u80fd\u91cf\u548c\u529b\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u8868\u73b0\u51fa\u7269\u7406\u4e00\u81f4\u7684\u6ce8\u610f\u529b\u6743\u91cd\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86GNN\u4e2d\u786c\u7f16\u7801\u56fe\u7684\u5fc5\u8981\u6027\uff0c\u8868\u660eTransformer\u53ef\u4ee5\u81ea\u9002\u5e94\u5b66\u4e60GNN\u7684\u6709\u5229\u7279\u6027\u3002"}}
