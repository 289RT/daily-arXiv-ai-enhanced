<div id=toc></div>

# Table of Contents

- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [1] [Cross-Chirality Generalization by Axial Vectors for Hetero-Chiral Protein-Peptide Interaction Design](https://arxiv.org/abs/2602.20176)
*Ziyi Yang,Zitong Tian,Yinjun Jia,Tianyi Zhang,Jiqing Zheng,Hao Wang,Yubu Su,Juncai He,Lei Liu,Yanyan Lan*

Main category: q-bio.BM

TL;DR: 通过注入轴向特征到E(3)-等变向量特征中，该方法实现了从同手性（L--L）训练数据到异手性（D--L）设计任务的跨手性泛化，并通过湿实验验证了设计的有效性。


<details>
  <summary>Details</summary>
Motivation: D-肽结合剂在治疗方面具有潜力，但目前机器学习在设计D-肽结合剂方面的应用仍未充分探索。

Method: 注入轴向特征到E(3)-等变向量特征中，并通过潜在扩散模型实现D-肽结合剂设计。

Result: 在计算机模拟和湿实验中均表现优异，是目前首个湿实验验证的生成式AI方法。

Conclusion: 该方法为处理蛋白质设计中的手性问题提供了新视角，并在D-肽结合剂设计中展现出潜力。

Abstract: D-peptide binders targeting L-proteins have promising therapeutic potential. Despite rapid advances in machine learning-based target-conditioned peptide design, generating D-peptide binders remains largely unexplored. In this work, we show that by injecting axial features to $E(3)$-equivariant (polar) vector features,it is feasible to achieve cross-chirality generalization from homo-chiral (L--L) training data to hetero-chiral (D--L) design tasks. By implementing this method within a latent diffusion model, we achieved D-peptide binder design that not only outperforms existing tools in in silico benchmarks, but also demonstrates efficacy in wet-lab validation. To our knowledge, our approach represents the first wet-lab validated generative AI for the de novo design of D-peptide binders, offering new perspectives on handling chirality in protein design.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference](https://arxiv.org/abs/2602.20449)
*Anna Hart,Chi Han,Jeonghwan Kim,Huimin Zhao,Heng Ji*

Main category: cs.LG

TL;DR: 论文研究了蛋白质语言模型（PLMs）与自然语言模型的差异，并提出一种早期退出技术，提升PLMs在蛋白质非结构属性预测中的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言与自然语言存在显著差异（如20种氨基酸的词汇），这些差异促使研究如何优化PLMs在蛋白质领域的应用。

Method: 直接比较蛋白质和自然语言域中信息分布的差异，并采用早期退出技术自动选择PLMs中间层表征以提高效率和准确性。

Result: 在非结构属性预测中，性能提升0.4-7.01个百分点，效率提高超10%。

Conclusion: 研究推动了蛋白质领域的语言模型优化，并为生物领域语言模型的发展提供了新方向。

Abstract: Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space despite a vocabulary of only 20 amino acids. These differences motivate research into how transformer-based architectures operate differently in the protein domain and how we can better leverage PLMs to solve protein-related tasks. In this work, we begin by directly comparing how the distribution of information stored across layers of attention heads differs between the protein and natural language domain. Furthermore, we adapt a simple early-exit technique-originally used in the natural language domain to improve efficiency at the cost of performance-to achieve both increased accuracy and substantial efficiency gains in protein non-structural property prediction by allowing the model to automatically select protein representations from the intermediate layers of the PLMs for the specific task and protein at hand. We achieve performance gains ranging from 0.4 to 7.01 percentage points while simultaneously improving efficiency by over 10 percent across models and non-structural prediction tasks. Our work opens up an area of research directly comparing how language models change behavior when moved into the protein domain and advances language modeling in biological domains.

</details>
