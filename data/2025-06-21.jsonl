{"id": "2506.14796", "pdf": "https://arxiv.org/pdf/2506.14796", "abs": "https://arxiv.org/abs/2506.14796", "authors": ["Zhangyang Gao", "Hao Wang", "Cheng Tan", "Chenrui Xu", "Mengdi Liu", "Bozhen Hu", "Linlin Chao", "Xiaoming Zhang", "Stan Z. Li"], "title": "PFMBench: Protein Foundation Model Benchmark", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": null, "summary": "This study investigates the current landscape and future directions of\nprotein foundation model research. While recent advancements have transformed\nprotein science and engineering, the field lacks a comprehensive benchmark for\nfair evaluation and in-depth understanding. Since ESM-1B, numerous protein\nfoundation models have emerged, each with unique datasets and methodologies.\nHowever, evaluations often focus on limited tasks tailored to specific models,\nhindering insights into broader generalization and limitations. Specifically,\nresearchers struggle to understand the relationships between tasks, assess how\nwell current models perform across them, and determine the criteria in\ndeveloping new foundation models. To fill this gap, we present PFMBench, a\ncomprehensive benchmark evaluating protein foundation models across 38 tasks\nspanning 8 key areas of protein science. Through hundreds of experiments on 17\nstate-of-the-art models across 38 tasks, PFMBench reveals the inherent\ncorrelations between tasks, identifies top-performing models, and provides a\nstreamlined evaluation protocol. Code is available at\n\\href{https://github.com/biomap-research/PFMBench}{\\textcolor{blue}{GitHub}}."}
{"id": "2506.15309", "pdf": "https://arxiv.org/pdf/2506.15309", "abs": "https://arxiv.org/abs/2506.15309", "authors": ["JÃºlia Vilalta-Mor", "Alexis Molina", "Laura Ortega Varga", "Isaac Filella-Merce", "Victor Guallar"], "title": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "16 pages, 7 figures", "summary": "Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes."}
{"id": "2506.15585", "pdf": "https://arxiv.org/pdf/2506.15585", "abs": "https://arxiv.org/abs/2506.15585", "authors": ["Robert Welch", "Charles Laughton", "Oliver Henrich", "Tom Burnley", "Daniel Cole", "Alan Real", "Sarah Harris", "James Gebbie-Rayet"], "title": "Engineering Supercomputing Platforms for Biomolecular Applications", "categories": ["physics.bio-ph", "q-bio.BM", "92-04", "J.2; J.3"], "comment": "54 pages, 55 figures. Benchmarking/method/software by Robert Welch.\n  MIG Benchmarks by Charles Laughton. Sections 1-6 by Robert Welch, 7 by James\n  Gebbie-Rayet & Robert Welch, 3.8 by Oliver Henrich. Edited by James\n  Gebbie-Rayet, Tom Burnley, Daniel Cole, Alan Real, Sarah Harris, Mark\n  Wilkinson. Methods: github.com/HECBioSim/hpcbench. Raw data:\n  github.com/HECBioSim/benchmark-results", "summary": "A range of computational biology software (GROMACS, AMBER, NAMD, LAMMPS,\nOpenMM, Psi4 and RELION) was benchmarked on a representative selection of HPC\nhardware, including AMD EPYC 7742 CPU nodes, NVIDIA V100 and AMD MI250X GPU\nnodes, and an NVIDIA GH200 testbed. The raw performance, power efficiency and\ndata storage requirements of the software was evaluated for each HPC facility,\nalong with qualitative factors such as the user experience and software\nenvironment. It was found that the diversity of methods used within\ncomputational biology means that there is no single HPC hardware that can\noptimally run every type of HPC job, and that diverse hardware is the only way\nto properly support all methods. New hardware, such as AMD GPUs and Nvidia AI\nchips, are mostly compatible with existing methods, but are also more\nlabour-intensive to support. GPUs offer the most efficient way to run most\ncomputational biology tasks, though some tasks still require CPUs. A fast HPC\nnode running molecular dynamics can produce around 10GB of data per day,\nhowever, most facilities and research institutions lack short-term and\nlong-term means to store this data. Finally, as the HPC landscape has become\nmore complex, deploying software and keeping HPC systems online has become more\ndifficult. This situation could be improved through hiring/training in DevOps\npractices, expanding the consortium model to provide greater support to HPC\nsystem administrators, and implementing build\nframeworks/containerisation/virtualisation tools to allow users to configure\ntheir own software environment, rather than relying on centralised software\ninstallations."}
