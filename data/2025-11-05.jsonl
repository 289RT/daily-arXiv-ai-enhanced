{"id": "2511.02128", "pdf": "https://arxiv.org/pdf/2511.02128", "abs": "https://arxiv.org/abs/2511.02128", "authors": ["Michael Chungyoun", "Gabe Au", "Britnie Carpentier", "Sreevarsha Puvada", "Courtney Thomas", "Jeffrey J. Gray"], "title": "DL4Proteins Jupyter Notebooks Teach how to use Artificial Intelligence for Biomolecular Structure Prediction and Design", "categories": ["q-bio.BM"], "comment": "27 pages, 5 figures", "summary": "Computational methods for predicting and designing biomolecular structures\nare increasingly powerful. While previous approaches relied on physics-based\nmodeling, modern tools, such as AlphaFold2 in CASP14, leverage artificial\nintelligence (AI) to achieve significantly improved performance. The growing\nimpact of AI-based tools in protein science necessitates enhanced educational\nmaterials that improve AI literacy among both established scientists seeking to\ndeepen their expertise and new researchers entering the field. To address this\nneed, we developed DL4Proteins, a series of ten interactive notebook modules\nthat introduce fundamental machine learning (ML) concepts, guide users through\ntraining ML models for protein-related tasks, and ultimately present\ncutting-edge protein structure prediction and design pipelines. With nothing\nmore than a web browser, learners can now access state-of-the-art computational\ntools employed by professional protein engineers - ranging from all-atom\nprotein design to fine-tuning protein language models for biophysically\nrelevant functional tasks. By increasing accessibility, this notebook series\nbroadens participation in AI-driven protein research. The complete notebook\nseries is publicly available at\nhttps://github.com/Graylab/DL4Proteins-notebooks."}
{"id": "2511.02622", "pdf": "https://arxiv.org/pdf/2511.02622", "abs": "https://arxiv.org/abs/2511.02622", "authors": ["Giuseppe Sacco", "Giovanni Bussi", "Guido Sanguinetti"], "title": "Machine Learning for RNA Secondary Structure Prediction: a review of current methods and challenges", "categories": ["q-bio.BM", "physics.bio-ph", "physics.comp-ph"], "comment": null, "summary": "Predicting the secondary structure of RNA is a core challenge in\ncomputational biology, essential for understanding molecular function and\ndesigning novel therapeutics. The field has evolved from foundational but\naccuracy-limited thermodynamic approaches to a new data-driven paradigm\ndominated by machine learning and deep learning. These models learn folding\npatterns directly from data, leading to significant performance gains. This\nreview surveys the modern landscape of these methods, covering single-sequence,\nevolutionary-based, and hybrid models that blend machine learning with\nbiophysics. A central theme is the field's \"generalization crisis,\" where\npowerful models were found to fail on new RNA families, prompting a\ncommunity-wide shift to stricter, homology-aware benchmarking. In response to\nthe underlying challenge of data scarcity, RNA foundation models have emerged,\nlearning from massive, unlabeled sequence corpora to improve generalization.\nFinally, we look ahead to the next set of major hurdles-including the accurate\nprediction of complex motifs like pseudoknots, scaling to kilobase-length\ntranscripts, incorporating the chemical diversity of modified nucleotides, and\nshifting the prediction target from static structures to the dynamic ensembles\nthat better capture biological function. We also highlight the need for a\nstandardized, prospective benchmarking system to ensure unbiased validation and\naccelerate progress."}
{"id": "2511.02769", "pdf": "https://arxiv.org/pdf/2511.02769", "abs": "https://arxiv.org/abs/2511.02769", "authors": ["Bum Chul Kwon", "Ben Shapira", "Moshiko Raboh", "Shreyans Sethi", "Shruti Murarka", "Joseph A Morrone", "Jianying Hu", "Parthasarathy Suryanarayanan"], "title": "STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "16 pages, 3 figures, 2 tables", "summary": "The chemical space of drug-like molecules is vast, motivating the development\nof generative models that must learn broad chemical distributions, enable\nconditional generation by capturing structure-property representations, and\nprovide fast molecular generation. Meeting the objectives depends on modeling\nchoices, including the probabilistic modeling approach, the conditional\ngenerative formulation, the architecture, and the molecular input\nrepresentation. To address the challenges, we present STAR-VAE\n(Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder),\na scalable latent-variable framework with a Transformer encoder and an\nautoregressive Transformer decoder. It is trained on 79 million drug-like\nmolecules from PubChem, using SELFIES to guarantee syntactic validity. The\nlatent-variable formulation enables conditional generation: a property\npredictor supplies a conditioning signal that is applied consistently to the\nlatent prior, the inference network, and the decoder. Our contributions are:\n(i) a Transformer-based latent-variable encoder-decoder model trained on\nSELFIES representations; (ii) a principled conditional latent-variable\nformulation for property-guided generation; and (iii) efficient finetuning with\nlow-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation\nwith limited property and activity data. On the GuacaMol and MOSES benchmarks,\nour approach matches or exceeds baselines, and latent-space analyses reveal\nsmooth, semantically structured representations that support both unconditional\nexploration and property-aware generation. On the Tartarus benchmarks, the\nconditional model shifts docking-score distributions toward stronger predicted\nbinding. These results suggest that a modernized, scale-appropriate VAE remains\ncompetitive for molecular generation when paired with principled conditioning\nand parameter-efficient finetuning."}
