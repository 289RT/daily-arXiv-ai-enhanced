{"id": "2602.13249", "pdf": "https://arxiv.org/pdf/2602.13249", "abs": "https://arxiv.org/abs/2602.13249", "authors": ["Hyosoon Jang", "Hyunjin Seo", "Yunhui Jang", "Seonghyun Park", "Sungsoo Ahn"], "title": "Boltz is a Strong Baseline for Atom-level Representation Learning", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": null, "summary": "Foundation models in molecular learning have advanced along two parallel tracks: protein models, which typically utilize evolutionary information to learn amino acid-level representations for folding, and small-molecule models, which focus on learning atom-level representations for property prediction tasks such as ADMET. Notably, cutting-edge protein-centric models such as Boltz now operate at atom-level granularity for protein-ligand co-folding, yet their atom-level expressiveness for small-molecule tasks remains unexplored. A key open question is whether these protein co-folding models capture transferable chemical physics or rely on protein evolutionary signals, which would limit their utility for small-molecule tasks. In this work, we investigate the quality of Boltz atom-level representations across diverse small-molecule benchmarks. Our results show that Boltz is competitive with specialized baselines on ADMET property prediction tasks and effective for molecular generation and optimization. These findings suggest that the representational capacity of cutting-edge protein-centric models has been underexplored and position Boltz as a strong baseline for atom-level representation learning for small molecules."}
{"id": "2602.13503", "pdf": "https://arxiv.org/pdf/2602.13503", "abs": "https://arxiv.org/abs/2602.13503", "authors": ["Maxwell Kleinsasser", "Brayden J. Halverson", "Edward Kraft", "Sean Francis-Lyon", "Sarah E. Hugo", "Mackenzie R. Roman", "Ben Miller", "Andrew D. Blevins", "Ian K. Quigley"], "title": "Hermes: Large DEL Datasets Train Generalizable Protein-Ligand Binding Prediction Models", "categories": ["q-bio.BM"], "comment": null, "summary": "The quality and consistency of training data remain critical bottlenecks for protein-ligand binding prediction. Public affinity datasets, aggregated from thousands of labs and assay formats, introduce biases that limit model generalization and complicate evaluation. DNA-encoded chemical libraries (DELs) offer a potential solution: unified experimental protocols generating massive binding datasets across diverse chemical and protein target space. We present Hermes, a lightweight transformer trained exclusively on DEL data from screens against hundreds of protein targets, representing one of the largest and most protein-diverse DEL training sets applied to protein-ligand interaction (PLI) modeling to date. Despite never seeing traditional affinity measurements during training, Hermes generalizes to held-out targets, novel chemical scaffolds, and external benchmarks derived from public binding data and high-throughput screens. Our results demonstrate that DEL data alone captures transferable protein-ligand interaction representations, while Hermes' minimal architecture enables inference speeds suitable for large-scale virtual screening."}
{"id": "2602.14005", "pdf": "https://arxiv.org/pdf/2602.14005", "abs": "https://arxiv.org/abs/2602.14005", "authors": ["Jiayi Wang", "Jules Nde", "Andrei G. Gasic", "Jacob Haseley", "Margaret S. Cheung"], "title": "Physical principles of building protein megacomplexes in a crowded milieu", "categories": ["q-bio.BM"], "comment": null, "summary": "Multiple phenotypic protein expressions arising from one genome represent variations in the protein relative abundance and their stoichiometry. A lack of definite compositional parts challenges the modeling of protein megacomplexes and cellular architectures. Despite the advances in protein structural predictions with AI, the mechanism of protein interactions and the emergence of megacomplexes they assemble remains unclear. Here, we present a statistical physics framework of grand canonical ensemble to explore the protein interactions that drive the emergent assembly of a megacomplex using the observational mass spectrometry datasets including protein relative abundance and the cross linked connections. Using chromatin remodeler megacomplex, INO80, as an example, we discovered a class of divergent protein that plays a critical role in orchestrating the assembly beyond nearest neighbors, dependent on the excluded volumes exerted by others. With the constraints of the excluded volumes by varying crowding contents, these divergent subunits orchestrate and form clusters with selective components growing into configurationally distinct architectures. We propose a machinery view for the INO80 chromatin remodeler complex where each loosely associated subunits can be occasionally recruited for parts as attachment into a core assembly driven by excluded volumes. Our computational framework provides a mechanistic insight into taking the macromolecular crowding as necessary physicochemical variables representing cell states to remodel the configurations of protein megacomplexes with structurally loose modules."}
{"id": "2602.14328", "pdf": "https://arxiv.org/pdf/2602.14328", "abs": "https://arxiv.org/abs/2602.14328", "authors": ["Slavica Jonic"], "title": "Conformational landscapes in cryo-ET data based on MD simulations", "categories": ["q-bio.BM", "q-bio.QM"], "comment": null, "summary": "Cryo-electron tomography (cryo-ET) provides a unique window into molecular organization in cellular environments (in situ). However, the interpretation of molecular structural information is complicated by several intrinsic properties of cryo-ET data, such as noise, missing wedge, and continuous conformational variability of the molecules. Additionally, in crowded in situ environments, the number of particles extracted is sometimes small and precludes extensive classification into discrete states. These challenges shift the emphasis from high-resolution structure determination toward validation and interpretation of low-resolution density maps, and analysis of conformational flexibility. Molecular Dynamics (MD) simulations are particularly well suited to this task, as they provide a physically grounded way to explore continuous conformation transitions consistent with both experimental data and molecular energetics. This review focuses on the roles of MD simulations in cryo-ET, emphasizing their use in emerging methods for conformational landscape determination and their contribution to gain new biological insight."}
{"id": "2602.13419", "pdf": "https://arxiv.org/pdf/2602.13419", "abs": "https://arxiv.org/abs/2602.13419", "authors": ["Shreyas Vinaya Sathyanarayana", "Shah Rahil Kirankumar", "Sharanabasava D. Hiremath", "Bharath Ramsundar"], "title": "Protect$^*$: Steerable Retrosynthesis through Neuro-Symbolic State Encoding", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG", "q-bio.BM"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable potential in scientific domains like retrosynthesis; yet, they often lack the fine-grained control necessary to navigate complex problem spaces without error. A critical challenge is directing an LLM to avoid specific, chemically sensitive sites on a molecule - a task where unconstrained generation can lead to invalid or undesirable synthetic pathways. In this work, we introduce Protect$^*$, a neuro-symbolic framework that grounds the generative capabilities of Large Language Models (LLMs) in rigorous chemical logic. Our approach combines automated rule-based reasoning - using a comprehensive database of 55+ SMARTS patterns and 40+ characterized protecting groups - with the generative intuition of neural models. The system operates via a hybrid architecture: an ``automatic mode'' where symbolic logic deterministically identifies and guards reactive sites, and a ``human-in-the-loop mode'' that integrates expert strategic constraints. Through ``active state tracking,'' we inject hard symbolic constraints into the neural inference process via a dedicated protection state linked to canonical atom maps. We demonstrate this neuro-symbolic approach through case studies on complex natural products, including the discovery of a novel synthetic pathway for Erythromycin B, showing that grounding neural generation in symbolic logic enables reliable, expert-level autonomy."}
{"id": "2602.15022", "pdf": "https://arxiv.org/pdf/2602.15022", "abs": "https://arxiv.org/abs/2602.15022", "authors": ["Cai Zhou", "Zijie Chen", "Zian Li", "Jike Wang", "Kaiyi Jiang", "Pan Li", "Rose Yu", "Muhan Zhang", "Stephen Bates", "Tommi Jaakkola"], "title": "Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation", "categories": ["cs.LG", "cs.AI", "math.GR", "q-bio.BM"], "comment": "32 pages", "summary": "Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \\times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation."}
