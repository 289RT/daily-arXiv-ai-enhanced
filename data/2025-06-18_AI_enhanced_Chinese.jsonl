{"id": "2506.14488", "pdf": "https://arxiv.org/pdf/2506.14488", "abs": "https://arxiv.org/abs/2506.14488", "authors": ["Dong Xu", "Zhangfan Yang", "Ka-chun Wong", "Zexuan Zhu", "Jiangqiang Li", "Junkai Ji"], "title": "Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced Aligned Diffusion", "categories": ["q-bio.BM", "cs.LG"], "comment": "13 pages, 5 figures", "summary": "Breakthroughs in high-accuracy protein structure prediction, such as AlphaFold, have established receptor-based molecule design as a critical driver for rapid early-phase drug discovery. However, most approaches still struggle to balance pocket-specific geometric fit with strict valence and synthetic constraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion termed READ is introduced, which is the first to merge molecular Retrieval-Augmented Generation with an SE(3)-equivariant diffusion model. Specifically, a contrastively pre-trained encoder aligns atom-level representations during training, then retrieves graph embeddings of pocket-matched scaffolds to guide each reverse-diffusion step at inference. This single mechanism can inject real-world chemical priors exactly where needed, producing valid, diverse, and shape-complementary ligands. Experimental results demonstrate that READ can achieve very competitive performance in CBGBench, surpassing state-of-the-art generative models and even native ligands. That suggests retrieval and diffusion can be co-optimized for faster, more reliable structure-based drug design.", "AI": {"tldr": "READ\u662f\u4e00\u79cd\u65b0\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548cSE(3)-\u7b49\u53d8\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u53d7\u4f53\u914d\u4f53\u8bbe\u8ba1\u4e2d\u7684\u51e0\u4f55\u5339\u914d\u4e0e\u5316\u5b66\u7ea6\u675f\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5728\u65e9\u671f\u836f\u7269\u53d1\u73b0\u4e2d\uff0c\u53d7\u4f53\u57fa\u5206\u5b50\u8bbe\u8ba1\u662f\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u51e0\u4f55\u5339\u914d\u548c\u5316\u5b66\u5408\u6210\u7ea6\u675f\u3002", "method": "READ\u9996\u6b21\u5c06\u5206\u5b50\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0eSE(3)-\u7b49\u53d8\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u901a\u8fc7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u5728\u63a8\u7406\u65f6\u68c0\u7d22\u53e3\u888b\u5339\u914d\u7684\u652f\u67b6\u4ee5\u6307\u5bfc\u6269\u6563\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eREAD\u5728CBGBench\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u751f\u6210\u6a21\u578b\u751a\u81f3\u539f\u751f\u914d\u4f53\u3002", "conclusion": "\u68c0\u7d22\u4e0e\u6269\u6563\u7684\u8054\u5408\u4f18\u5316\u4e3a\u57fa\u4e8e\u7ed3\u6784\u7684\u836f\u7269\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
