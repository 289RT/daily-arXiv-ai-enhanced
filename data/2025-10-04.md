<div id=toc></div>

# Table of Contents

- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [1] [BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning](https://arxiv.org/abs/2510.01632)
*Xin Wang,Carlos Oliver*

Main category: q-bio.BM

TL;DR: BioBlobs是一种动态分区蛋白质结构的模块，通过量化生成功能相关的蛋白质子结构词汇，提升蛋白质表示学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质表示学习模型依赖固定子结构（如k-hop或固定半径邻域），忽略了蛋白质功能驱动的多变子结构信号。BioBlobs旨在通过动态分区更好地捕捉这些功能相关子结构。

Method: BioBlobs通过动态分区将蛋白质结构划分为大小灵活、不重叠的子结构（“blobs”），并量化到共享且可解释的代码本中，生成功能相关的蛋白质子结构词汇。

Result: BioBlobs显著提升了广泛使用的蛋白质编码器（如GVP-GNN）在多种任务中的性能，同时提供了蛋白质功能的机制性见解。

Conclusion: BioBlobs的直接捕捉功能相关子结构的架构不仅能提高预测性能，还能提供对蛋白质功能的深入理解。

Abstract: Protein function is driven by coherent substructures which vary in size and
topology, yet current protein representation learning models (PRL) distort
these signals by relying on rigid substructures such as k-hop and fixed radius
neighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable
module that represents proteins by dynamically partitioning structures into
flexibly-sized, non-overlapping substructures ("blobs"). The resulting blobs
are quantized into a shared and interpretable codebook, yielding a discrete
vocabulary of function-relevant protein substructures used to compute protein
embeddings. We show that BioBlobs representations improve the performance of
widely used protein encoders such as GVP-GNN across various PRL tasks. Our
approach highlights the value of architectures that directly capture
function-relevant protein substructures, enabling both improved predictive
performance and mechanistic insight into protein function.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: 研究者探讨了强化学习（RL）是否能帮助蛋白质语言模型（PLMs）超越预训练先验，发现潜在的序列-结构-功能规则。实验表明，RL能显著提高成功率和采样效率，但其效果取决于任务空间、奖励准确性和策略能力。


<details>
  <summary>Details</summary>
Motivation: 探索RL是否能推动PLMs超越其预训练先验，揭示潜在的序列-结构-功能规则，特别是通过结合RL与PLMs在多领域蛋白质设计中。

Method: 通过结合RL与PLMs，在抗菌肽设计、激酶变体优化、抗体工程和逆折叠四个领域进行实验，并使用多种RL算法和模型类别进行研究。

Result: RL显著提高了成功率和采样效率，但其效果受任务空间、奖励准确性和策略能力的共同影响。当奖励准确、任务有优化空间且策略能力足够时，改进显著；反之则效果有限。

Conclusion: RL在蛋白质设计中具有实际应用价值，但需优先优化奖励模型和校准，并根据任务难度选择合适的算法和正则化强度。

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [3] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman,Yutong Bai,Fadi Atieh,Elizabeth Weaver,Eric Qu,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 研究探讨了未经修改的Transformer是否能直接处理笛卡尔坐标预测分子能量和力，挑战了GNN的主导地位。


<details>
  <summary>Details</summary>
Motivation: GNN在分子机器学习中虽主导，但其固定的图结构和稀疏操作限制了表达力和推理速度，研究者希望验证Transformer是否能在无预设图或物理先验的情况下表现优越。

Method: 使用标准Transformer直接训练笛卡尔坐标数据，对比GNN在同等计算预算下的表现。

Result: Transformer在OMol25数据集上达到与GNN竞争的能量和力预测精度，并表现出物理一致性（如注意力权重随原子距离衰减）。

Conclusion: Transformer无需硬编码图偏置即可自适应学习分子任务特性，为分子建模提供了标准化、可扩展的架构选择。

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular
machine learning, particularly for molecular property prediction and machine
learning interatomic potentials (MLIPs). GNNs perform message passing on
predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor
scheme. While this design aligns with the locality present in many molecular
tasks, a hard-coded graph can limit expressivity due to the fixed receptive
field and slows down inference with sparse graph operations. In this work, we
investigate whether pure, unmodified Transformers trained directly on Cartesian
coordinates$\unicode{x2013}$without predefined graphs or physical
priors$\unicode{x2013}$can approximate molecular energies and forces. As a
starting point for our analysis, we demonstrate how to train a Transformer to
competitive energy and force mean absolute errors under a matched training
compute budget, relative to a state-of-the-art equivariant GNN on the OMol25
dataset. We discover that the Transformer learns physically consistent
patterns$\unicode{x2013}$such as attention weights that decay inversely with
interatomic distance$\unicode{x2013}$and flexibly adapts them across different
molecular environments due to the absence of hard-coded biases. The use of a
standard Transformer also unlocks predictable improvements with respect to
scaling training resources, consistent with empirical scaling laws observed in
other domains. Our results demonstrate that many favorable properties of GNNs
can emerge adaptively in Transformers, challenging the necessity of hard-coded
graph inductive biases and pointing toward standardized, scalable architectures
for molecular modeling.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [4] [Folding lattice proteins confined on minimal grids using a quantum-inspired encoding](https://arxiv.org/abs/2510.01890)
*Anders Irbäck,Lucas Knuthson,Sandipan Mohanty*

Main category: quant-ph

TL;DR: 通过将密集蛋白质系统中的空间冲突问题转化为QUBO问题，研究者展示了经典模拟退火和混合量子经典退火方法可以有效解决链长为48的问题，混合方法仅需约10秒。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理密集蛋白质系统中的空间冲突问题时效率低下，需要找到更有效的解决方案。

Method: 将问题转化为QUBO形式，并测试经典模拟退火、混合量子经典退火、线性和二次规划方法，同时与穷举结构枚举结果对比。

Result: 经典模拟退火和混合量子经典退火方法能快速一致地解决问题，混合方法仅需10秒；线性和二次规划方法在链约束下表现不佳。

Conclusion: QUBO形式的问题适用于经典和量子方法，混合量子经典退火展现出高效率。

Abstract: Steric clashes pose a challenge when exploring dense protein systems using
conventional explicit-chain methods. A minimal example is a single lattice
protein confined on a minimal grid, with no free sites. Finding its minimum
energy is a hard optimization problem, withsimilarities to scheduling problems.
It can be recast as a quadratic unconstrained binary optimization (QUBO)
problem amenable to classical and quantum approaches. We show that this problem
in its QUBO form can be swiftly and consistently solved for chain length 48,
using either classical simulated annealing or hybrid quantum-classical
annealing on a D-Wave system. In fact, the latter computations required about
10 seconds. We also test linear and quadratic programming methods, which work
well for a lattice gas but struggle with chain constraints. All methods are
benchmarked against exact results obtained from exhaustive structure
enumeration, at a high computational cost.

</details>
