{"id": "2506.08023", "pdf": "https://arxiv.org/pdf/2506.08023", "abs": "https://arxiv.org/abs/2506.08023", "authors": ["Qifeng Wu", "Zhengzhe Liu", "Han Zhu", "Yizhou Zhao", "Daisuke Kihara", "Min Xu"], "title": "Aligning Proteins and Language: A Foundation Model for Protein Retrieval", "categories": ["q-bio.BM", "cs.AI", "cs.CE", "cs.CV", "cs.LG"], "comment": "4 pages for body, 3 pages for appendix, 11 figures. Accepted to CVPR\n  2025 Workshop on Multimodal Foundation Models for Biomedicine: Challenges and\n  Opportunities(MMFM-BIOMED)", "summary": "This paper aims to retrieve proteins with similar structures and semantics\nfrom large-scale protein dataset, facilitating the functional interpretation of\nprotein structures derived by structural determination methods like\ncryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of\nvision-language models (VLMs), we propose a CLIP-style framework for aligning\n3D protein structures with functional annotations using contrastive learning.\nFor model training, we propose a large-scale dataset of approximately 200,000\nprotein-caption pairs with rich functional descriptors. We evaluate our model\nin both in-domain and more challenging cross-database retrieval on Protein Data\nBank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In\nboth cases, our approach demonstrates promising zero-shot retrieval\nperformance, highlighting the potential of multimodal foundation models for\nstructure-function understanding in protein biology."}
{"id": "2506.08293", "pdf": "https://arxiv.org/pdf/2506.08293", "abs": "https://arxiv.org/abs/2506.08293", "authors": ["Logan Hallee", "Nikolaos Rafailidis", "David B. Bichara", "Jason P. Gleghorn"], "title": "Diffusion Sequence Models for Enhanced Protein Representation and Generation", "categories": ["q-bio.BM"], "comment": "20 pages, 15 figures", "summary": "Proteins are fundamental to biology, executing diverse functions through\ncomplex physicochemical interactions, and they hold transformative potential\nacross medicine, materials science, and environmental applications. Protein\nLanguage Models (pLMs) aim to unlock insights from the vast space of unlabeled\nprotein sequences by learning rich, semantic representations from primary\nsequences via masked language modeling. However, these models typically exhibit\nlimited generative capacity. In this work, we introduce the Diffusion Sequence\nModel (DSM), a novel pLM trained with masked diffusion to enable both\nhigh-quality representation learning and generative protein design. DSM builds\nupon the ESM2 architecture by incorporating a masked forward diffusion process\ninspired by the LLaDA framework. After training, DSM is capable of generating\ndiverse, biomimetic sequences that align with expected amino acid compositions,\nsecondary structures, and predicted functions, even with 90\\% token corruption.\nFurthermore, DSM's learned representations match or exceed those of similarly\nsized pLMs on downstream tasks. We also introduce DSM(ppi), a variant\nfine-tuned to generate protein binders by attending to target sequences. We\ndemonstrate DSM(ppi)'s effectiveness on the challenging Bench-tested Binder\nBenchmark (BenchBB), where both DSM and DSM(ppi) produce candidates with\nsuperior predicted binding affinity compared to known binders. Our results\nestablish masked diffusion as a powerful paradigm for unifying protein\nrepresentation and generation in a single framework."}
{"id": "2506.08365", "pdf": "https://arxiv.org/pdf/2506.08365", "abs": "https://arxiv.org/abs/2506.08365", "authors": ["Cheng Tan", "Zhenxiao Cao", "Zhangyang Gao", "Siyuan Li", "Yufei Huang", "Stan Z. Li"], "title": "AlphaFold Database Debiasing for Robust Inverse Folding", "categories": ["cs.LG", "q-bio.BM"], "comment": "Under review", "summary": "The AlphaFold Protein Structure Database (AFDB) offers unparalleled\nstructural coverage at near-experimental accuracy, positioning it as a valuable\nresource for data-driven protein design. However, its direct use in training\ndeep models that are sensitive to fine-grained atomic geometry, such as inverse\nfolding, exposes a critical limitation. Comparative analysis of structural\nfeature distributions reveals that AFDB structures exhibit distinct statistical\nregularities, reflecting a systematic geometric bias that deviates from the\nconformational diversity found in experimentally determined structures from the\nProtein Data Bank (PDB). While AFDB structures are cleaner and more idealized,\nPDB structures capture the intrinsic variability and physical realism essential\nfor generalization in downstream tasks. To address this discrepancy, we\nintroduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct\nnative-like conformations from intentionally corrupted backbone geometries. By\ntraining the model to recover plausible structural states, DeSAE implicitly\ncaptures a more robust and natural structural manifold. At inference, applying\nDeSAE to AFDB structures produces debiased structures that significantly\nimprove inverse folding performance across multiple benchmarks. This work\nhighlights the critical impact of subtle systematic biases in predicted\nstructures and presents a principled framework for debiasing, significantly\nboosting the performance of structure-based learning tasks like inverse\nfolding."}
